# -*- coding: utf-8 -*-
"""solution.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RdE5_ZIBJdQJPzFqzICo9JR7GF68SNYz

# IFT6135-A2022
# Assignment 1: Practical

Vous devez remplir vos réponses dans ce Notebook, après quoi vous devez exporter ce Notebook vers un fichier Python nommé `solution.py` et le soumettre sur Gradescope.

N'éditez pas les fonctions spécifiées dans le PDF (et partout où cela est indiqué - `# WRITE CODE HERE`). Ne changez pas les définitions et ne modifiez pas le reste du modèle, sinon l'autograde ne fonctionnera pas.

**Assurez-vous de demander un runtime GPU (surtout pour la question 3) !**
"""

# DO NOT MODIFY!
import matplotlib.pyplot as plt
import numpy as np
import random

# Fix random seed
random.seed(0)
np.random.seed(0)

"""## Question 1: Implementing MLPs with NumPy (30 points)"""

class NN(object):
  """
    Implements an MLP.
  """

  def __init__(self,
               dims=(784, 128, 64, 10), # h_0, h_1, h_2, h_3
               activation="relu",       # Activation function
               epsilon=1e-6,            # Correction factor
               lr=0.01,                 # Learning rate
               seed=0                   # Random seed
              ):
    """
      Constructor of the NN class.

      dims: list or tuple or np.array, default (784, 128, 64, 10)
        Valeurs de h_0 (nombre de caractéristiques), h_1 (dim. cachée 1), h_2 (dim. cachée 2), h_3 (nombre de classes de sortie).
      activation: string, default "relu"
        La fonction d'activation.
      epsilon: float or double, default 1e-6
        Facteur de correction des probabilités.
      lr: float or double, default 0.01
        Taux d'apprentissage.
      seed: int, default 0
        Random seed.
    """
    super(NN, self).__init__()

    self.dims = dims
    self.n_hidden = len(dims) - 2
    self.activation_str = activation
    self.epsilon = epsilon
    self.lr = lr
    self.seed = seed

  def initialize_weights(self):
    """
      Résultats : Initialise les weights du MLP à partir de uniform(-1/sqrt(h_0), 1/sqrt(h_0)) et les biais à zéro.
    """
    if self.seed is not None:
      np.random.seed(self.seed)

    self.weights = {}
    # self.weights is a dictionary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers
    # The keys W1, W2, ..., Wm correspond to weights while b1, b2, ..., bm correspond to biases
    for layer_n in range(1, self.n_hidden + 2):
      # WRITE CODE HERE
      self.weights[f"W{layer_n}"] = np.random.uniform(low = -1/np.sqrt(self.dims[0]), high = 1/np.sqrt(self.dims[0]), size = (self.dims[layer_n - 1], self.dims[layer_n]))
      self.weights[f"b{layer_n}"] = np.zeros((1, self.dims[layer_n]))

  def relu(self, x, grad=False):
    """
      x: np.array
        Entrées pour lesquelles calculer ReLU(x). x peut contenir un batch d'entrées !
      grad: bool, default False
        Si True, retourne le gradient de l'activation par rapport aux entrées de la fonction.

      Outputs: Implémente la fonction d'activation ReLU ou son gradient.
    """
    if grad:
      return (x > 0) * 1
    return np.maximum(0,x)

  def sigmoid(self, x, grad=False):
    """
      x: np.array
        Entrées pour lesquelles calculer sigmoid(x). x peut contenir un batch d'entrées !
      grad: bool, default False
        Si True, retourne le gradient de l'activation par rapport aux entrées de la fonction.

      Outputs: Implémente la fonction d'activation sigmoid ou son gradient.
    """
    if grad:
      return np.exp(-x)/(1 + np.exp(-x))**2
    return 1 / (1 + np.exp(-x)) 

  def tanh(self, x, grad=False):
    """
      x: np.array
        Entrées pour lesquelles calculer tanh(x). x peut contenir un batch d'entrées !
      grad: bool, default False
        Si True, retourne le gradient de l'activation par rapport aux entrées de la fonction.

      Outputs: Implémente la fonction d'activation tanh ou son gradient.
    """
    if grad:
      return 1 - ((1 - np.exp(-2*x))/(1 + np.exp(-2*x)))**2
    return (1 - np.exp(-2*x))/(1 + np.exp(-2*x))

  def activation(self, x, grad=False):
    """
      x: np.array
        Entrées pour calculer l'activation(x). x peut contenir un batch d'entrées !
      grad: bool, default False
        Si True, retourne le gradient de l'activation par rapport aux entrées de la fonction.

      Outputs: Retourne la valeur de l'activation ou le gradient.
    """
    if self.activation_str == "relu":
      return self.relu(x,grad)
    elif self.activation_str == "sigmoid":
      return self.sigmoid(x,grad)
    elif self.activation_str == "tanh":
      return self.tanh(x,grad)
    else:
      raise Exception("Invalid activation")

  def softmax(self, x):
    """
      x: np.array
        Entrées pour calculer le softmax. x peut contenir un batch d'entrées !

      Outputs: Implémente la fonction softmax, renvoie l'array contenant softmax(x).
    """
    # Remember that softmax(x-C) = softmax(x) when C is a constant.
    # WRITE CODE HERE
    res = np.array([])
    for i in range(x.shape[0]):
      res = np.append(res, np.exp(x[i] - np.max(x[i]))/np.sum(np.exp(x[i] - np.max(x[i]))))
    return res.reshape(x.shape)

  def forward(self, x):
    """
      x: np.array
        Entrées du MLP. Notez que x peut contenir plusieurs exemples d'entrée.

      Outputs: Implémente la passe forward, retourne le cache comme décrit ci-dessous.
    """
    cache = {"Z0": x}
    # le cache est un dictionnaire avec les clés Z0, A1, Z1, ..., Am, Zm où m - 1 est le nombre de couches cachées
    # Z0 contient juste les entrées x du réseau
    # Ai correspond à la préactivation de la couche i, Zi correspond à l'activation de la couche i
    for layer_n in range(1, self.n_hidden + 2):
      cache[f"A{layer_n}"] = np.dot(cache[f"Z{layer_n - 1}"], self.weights[f"W{layer_n}"]) + self.weights[f"b{layer_n}"]
      if layer_n == self.n_hidden + 1:
        cache[f"Z{layer_n}"] = self.softmax(cache[f"A{layer_n}"]) 
      else:
        cache[f"Z{layer_n}"] = self.activation(cache[f"A{layer_n}"])
    return cache

  def loss(self, prediction, labels):
    """
      prediction: np.array
        Probabilités prédites pour chaque classe pour les entrées. Peut contenir plusieurs exemples (un batch) !
      labels: np.array
        Vraies labels correspondant aux entrées (one-hot encoded). Peut contenir plusieurs exemples (un batch) !

      Outputs: Renvoie la perte de l'entropie croisée (moyenne sur le nombre d'entrées).
    """
    prediction[np.where(prediction < self.epsilon)] = self.epsilon
    prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon
    return -np.mean(np.sum(labels * np.log(prediction), axis=1))

  def backward(self, cache, labels):
    """
      cache: np.array
        Résultats de la passe backward. Cela peut être pour plusieurs exemples (un batch).
      labels: np.array
        Vraies labels correspondant aux entrées dans le cache. Peut contenir plusieurs exemples (un batch) !

      Outputs: Implémente le backward pass, retourne les grads comme décrit ci-dessous.
    """
    output = cache[f"Z{self.n_hidden + 1}"]
    grads = {}

    # grads est un dictionnaire avec les clés dAm, dWm, dbm, dZ(m-1), dA(m-1), ..., dW1, db1
    # N'oubliez pas de prendre en compte le nombre d'exemples en entrée !
    for layer_n in range(self.n_hidden + 1, 0, -1):
      if layer_n == self.n_hidden + 1:
        grads[f"dA{layer_n}"] = output - labels
      else:
        grads[f"dA{layer_n}"] = grads[f"dZ{layer_n}"] * self.activation(cache[f"A{layer_n}"], grad=True)
      grads[f"dW{layer_n}"] = np.dot(cache[f"Z{layer_n - 1}"].T, grads[f"dA{layer_n}"]) / labels.shape[0]
      grads[f"db{layer_n}"] = grads[f"dA{layer_n}"].mean(axis=0, keepdims=True)
      if layer_n > 1:
        grads[f"dZ{layer_n - 1}"] = np.dot(grads[f"dA{layer_n}"], self.weights[f"W{layer_n}"].T)
    return grads
    
  def update(self, grads):
    """
      grads: np.dictionary
        Gradients obtenus à partir de la passe backward.

      Results: Met à jour les poids et les biais du réseau.
    """
    for layer in range(1, self.n_hidden + 2):
      self.weights[f"W{layer}"] -= self.lr * grads[f"dW{layer}"]
      self.weights[f"b{layer}"] -= self.lr * grads[f"db{layer}"]

"""## Question 2: Implementing CNN layers with NumPy (20 points)
Note: You may assume that there are no biases, no input padding (valid convolution) and also that convolution here refers to cross-correlation, i.e., no kernel flipping when convolving the inputs.
"""

class Convolution2dLayer(object):
  """
    Implements a 2D convolution layer.
  """

  def __init__(self, filter_size=3, stride=1, n_units=64, seed=0):
    """
      Constructor of the Convolution2dLayer class.

      Note: Nous supposons que les images d'entrée n'ont qu'un seul canal.

      filter_size : int, default 3
        Taille du filtre à utiliser pour la convolution. Nous supposons que la hauteur et la largeur sont égales.
      stride : int, default 1
        Stride pour la convolution.
      n_units : int, default 64
        Nombre de canaux de sortie, c'est à dire le nombre de filtres dans la couche.
      seed : int, default 0
        Random seed.
    """
    super(Convolution2dLayer, self).__init__()

    self.filter_size = filter_size
    self.stride = stride
    self.n_units = n_units
    self.seed = seed

  def initialize_weights(self):     
    """
      Results: Initialise les poids du CNN à partir de uniform(0, 1).
    """   
    if self.seed is not None:
      np.random.seed(self.seed)

    # self.weights est un tableau np.array de forme (n_units, filter_size, filter_size)
    # Nous ne considérons pas les biais dans cette implémentation de la couche de convolution.
    # WRITE CODE HERE
    self.weights = np.random.uniform(0, 1, (self.n_units, self.filter_size, self.filter_size))

  def forward(self, x):
    """
      x: np.array
        Entrées. Ceci peut contenir plusieurs exemples d'entrée, pas seulement un.
        Note : Nous supposons que les images d'entrée n'ont qu'un seul canal, par exemple (5, 1, 32, 32) où 5 est le nombre d'images, 1 canal, taille d'image 32x32.
        images, 1 canal, taille de l'image 32x32.

      Outputs: Les entrées et le résultat de l'opération de convolution sur les entrées stockées dans le cache.

      Note: Vous n'avez pas besoin de 'flip' le noyau ! Vous pouvez simplement implémenter la corrélation croisée.
    """
    cache = {}

    # cache est un dictionnaire où cache["x"] stocke les entrées et cache["out"] stocke les sorties de la couche
    # WRITE CODE HERE
    cache["x"] = x
    n_images, _, height, width = x.shape
    out_height = (height - self.filter_size) // self.stride + 1
    out_width = (width - self.filter_size) // self.stride + 1
    out = np.zeros((n_images, self.n_units, out_height, out_width))
    for image_n in range(n_images):
      for unit_n in range(self.n_units):
        for h in range(out_height):
          for w in range(out_width):
            out[image_n, unit_n, h, w] = np.sum(x[image_n, :, h * self.stride:h * self.stride + self.filter_size, w * self.stride:w * self.stride + self.filter_size] * self.weights[unit_n])
    cache["out"] = out
    return cache


  def backward(self, cache, grad_output):
    """
      cache: dictionary
        Contient les entrées et le résultat de l'opération de convolution qui leur est appliquée.
      grad_output: np.array
        Gradient de la perte par rapport aux sorties de la couche de convolution.

      Outputs: Gradient de la perte par rapport aux paramètres de la couche de convolution.
    """
    # grads est un tableau np.array contenant le gradient de la perte par rapport aux paramètres de la couche de convolution.
    # N'oubliez pas de prendre en compte le nombre d'exemples en entrée !
    # WRITE CODE HERE
    grads = np.zeros(self.weights.shape)
    x = cache["x"]
    n_images, _, height, width = x.shape
    out_height = (height - self.filter_size) // self.stride + 1
    out_width = (width - self.filter_size) // self.stride + 1
    for image_n in range(n_images):
      for unit_n in range(self.n_units):
        for h in range(out_height):
          for w in range(out_width):
            grads[unit_n] = grads[unit_n] + grad_output[image_n, unit_n, h, w] * x[image_n, :, h * self.stride:h * self.stride + self.filter_size, w * self.stride:w * self.stride + self.filter_size]
    return grads




class MaxPooling2dLayer(object):
  """
    Implements a 2D max-pooling layer.
  """

  def __init__(self, filter_size=2):
    """
      Constructor of the MaxPooling2dLayer class.

      filter_size: int, default 2
        Taille du filtre à utiliser pour le max-pooling. Nous supposons que la hauteur et la largeur sont égales, et que le pas = hauteur = largeur.
    """
    super(MaxPooling2dLayer, self).__init__()
  
    self.filter_size = filter_size
    self.stride = filter_size

  def forward(self, x):
    """
      x: np.array
        Entrées pour lesquelles calculer le max-pooling. Ceci peut contenir plusieurs exemples d'entrée, pas seulement un.
        Note : Les dimensions d'entrée du max-pooling sont les dimensions de sortie de la convolution !

      Outputs: Les entrées et le résultat de l'opération de max-pooling sur les entrées stockées dans le cache.
    """
    cache = {}

    # cache est un dictionnaire où cache["x"] stocke les entrées et cache["out"] stocke les sorties de la couche
    # WRITE CODE HERE
    cache["x"] = x
    n_images, n_units, height, width = x.shape
    out_height = (height - self.filter_size) // self.stride + 1
    out_width = (width - self.filter_size) // self.stride + 1
    out = np.zeros((n_images, n_units, out_height, out_width))
    for image_n in range(n_images):
      for unit_n in range(n_units):
        for h in range(out_height):
          for w in range(out_width):
            out[image_n, unit_n, h, w] = np.max(x[image_n, unit_n, h * self.stride:h * self.stride + self.filter_size, w * self.stride:w * self.stride + self.filter_size])
    cache["out"] = out
    return cache

  def backward(self, cache, grad_output):
    """
      cache: dictionary
        Contient les entrées et le résultat de l'opération de max-pooling qui leur est appliquée.
      grad_output: np.array
        Gradient de la perte par rapport aux sorties de la couche de max-pooling.

      Outputs: Gradient de la perte par rapport aux entrées de la couche de max-pooling.
    """
    x = cache["x"]
    grads = np.zeros_like(x) # WRITE CODE HERE (initialize grads correctly)

    # grads est un tableau np.array contenant le gradient de la perte par rapport aux entrées de la couche de max-pooling.
    # N'oubliez pas de prendre en compte le nombre d'exemples en entrée !
    # WRITE CODE HERE
    n_images, n_units, height, width = x.shape
    out_height = (height - self.filter_size) // self.stride + 1
    out_width = (width - self.filter_size) // self.stride + 1
    for image_n in range(n_images):
      for unit_n in range(n_units):
        for h in range(out_height):
          for w in range(out_width):
            max_val = np.max(x[image_n, unit_n, h * self.stride:h * self.stride + self.filter_size, w * self.stride:w * self.stride + self.filter_size])
            for h_ in range(self.filter_size):
              for w_ in range(self.filter_size):
                if x[image_n, unit_n, h * self.stride + h_, w * self.stride + w_] == max_val:
                  grads[image_n, unit_n, h * self.stride + h_, w * self.stride + w_] = grad_output[image_n, unit_n, h, w]
    return grads

"""## Question 3: Implementing a CNN and comparison with MLPs using PyTorch (50 points)"""

# DO NOT MODIFY!
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
device = "cuda" if torch.cuda.is_available() else "cpu"

# Fix random seed
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
torch.cuda.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

class ResidualBlock(nn.Module):
  """This class implements the Residual Block used in ResNet-18."""

  def __init__(self, in_channels, channels, conv_stride=1, activation_str="relu", initialization="xavier_normal"):
    """
      Constructor for the ResidualBlock class.

      in_channels : int
        Nombre de canaux dans l'entrée du bloc.
      channels : int
        Nombre de canaux de sortie pour le bloc, c'est-à-dire le nombre de filtres.
      conv_stride : int, par défaut 1
        Stride de la première couche de convolution et de la convolution de sous-échantillonnage (si nécessaire).
      activation_str : string, par défaut "relu" (chaîne de caractères)
        Fonction d'activation à utiliser.
      initialization : string, par défaut "xavier_normal".
        Initialisation des poids de la couche de convolution.
    """
    super(ResidualBlock, self).__init__()

    self.in_channels = in_channels
    self.channels = channels
    self.conv_stride = conv_stride
    self.activation_str = activation_str
    self.initialization = initialization

    # Define these members by replacing `None` with the correct definitions
    self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=(3,3), stride=conv_stride, padding = (1,1), bias = False)
    self.bn1 = nn.BatchNorm2d(channels)
    self.conv2 = nn.Conv2d(channels, channels, kernel_size=(3,3), stride=(1,1), padding = (1,1), bias = False)
    self.bn2 = nn.BatchNorm2d(channels)

    self.residual_connection = self.residual(in_channels, channels, conv_stride)

    # Initialize weights for conv1 and conv2
    if initialization == "xavier_normal":
      init.xavier_normal_(self.conv1.weight)
      init.xavier_normal_(self.conv2.weight)
    elif initialization == "xavier_uniform":
      init.xavier_uniform_(self.conv1.weight)
      init.xavier_uniform_(self.conv2.weight)
    elif initialization == "kaiming_normal":
      init.kaiming_normal_(self.conv1.weight)
      init.kaiming_normal_(self.conv2.weight)
    else:
      raise Exception("Invalid initialization")

  def activation(self, input):
    """
      entrée : Tensor
        Entrée sur laquelle l'activation est appliquée.

      Sortie : Résultat de la fonction d'activation appliquée à l'entrée.
        Par exemple, si self.activation_str est "relu", retourne relu(input).
    """
    if self.activation_str == "relu":
      return nn.ReLU()(input)
    elif self.activation_str == "tanh":
      return nn.Tanh()(input)
    else:
      raise Exception("Invalid activation")

  def residual(self, in_channels, channels, conv_stride=1):
    """
      in_channels : int
        Nombre de canaux d'entrée dans l'entrée du bloc.
      channels : int
        Nombre de canaux de sortie pour le bloc, c'est-à-dire le nombre de filtres.
      conv_stride : int, par défaut 1
        Stride à utiliser pour le sous-échantillonnage de la convolution 1x1.

      Sortie : Retourne un objet nn.Sequential qui calcule la fonction d'identité de l'entrée si stride est 1
              et que le nombre de canaux d'entrée est égal au nombre de canaux de sortie. Sinon, il retourne un objet
              nn.Sequential qui sous-échantillonne son entrée en utilisant un 1x1-conv du stride spécifié et un BatchNorm2.
              suivi d'un BatchNorm2d.
    """
    layers = []
    if conv_stride != 1 or in_channels != channels:
      layers.append(nn.Conv2d(in_channels, channels, kernel_size=(1,1), stride=conv_stride, bias = False))
      layers.append(nn.BatchNorm2d(channels))
    return nn.Sequential(*layers)

  def forward(self, x):
    """
      x : Tensor
        Entrée du bloc.

      Sorties : Renvoie la sortie de la passe avant du bloc.
    """
    return self.activation(self.bn2(self.conv2(self.activation(self.bn1(self.conv1(x))))) + self.residual_connection(x))

class ResNet18(nn.Module):
  """This class implements the ResNet-18 architecture from its components."""

  def __init__(self, activation_str="relu", initialization="xavier_normal"):
    """
      Constructor for the ResNet18 class.

      activation_str : string, par défaut "relu".
        Fonction d'activation à utiliser.
      initialization : string, par défaut "xavier_normal".
        Initialisation du poids à utiliser.
    """
    super(ResNet18, self).__init__()

    self.n_classes = 10
    self.activation_str = activation_str
    self.initialization = initialization

    # Define these members by replacing `None` with the correct definitions
    self.conv1 = nn.Conv2d(3, 64, kernel_size=(3,3), stride=(1,1), padding = (1,1), bias = False)
    self.bn1 = nn.BatchNorm2d(64)
    self.layer1 = self._create_layer(64, 64)
    self.layer2 = self._create_layer(64, 128, 2)
    self.layer3 = self._create_layer(128, 256, 2)
    self.layer4 = self._create_layer(256, 512, 2)
    self.avgpool = nn.AvgPool2d(kernel_size=(4,4), stride=(1,1))
    self.linear = nn.Linear(512, self.n_classes)
  
  def activation(self, input):
    """
      input: Tensor
        Entrée sur laquelle l'activation est appliquée.

      Sortie : Résultat de la fonction d'activation appliquée à l'entrée.
        Par exemple, si self.activation_str est "relu", retourne relu(input).
    """
    if self.activation_str == "relu":
      return nn.ReLU()(input)
    elif self.activation_str == "tanh":
      return nn.Tanh()(input)
    else:
      raise Exception("Invalid activation")

  def _create_layer(self, in_channels, channels, conv_stride=1):
    """
      in_channels: int
        Number of input channels present in the input to the layer.
      out_channels: int
        Number of output channels for the layer, i.e., the number of filters.
      conv_stride: int, default 1
        Stride of the first convolution layer in the block and the downsampling convolution (if required).

      Outputs: Returns an nn.Sequential object giving a "layer" of the ResNet, consisting of 2 blocks each.
    """
    # Modifiez l'instruction suivante pour retourner un objet nn.Sequential contenant 2 ResidualBlocks.
    # Vous devez vous assurer que les canaux et conv_stride appropriés sont fournis.
    return nn.Sequential(ResidualBlock(in_channels, channels, conv_stride, self.activation_str, self.initialization), ResidualBlock(channels, channels, 1, self.activation_str, self.initialization))

  def get_first_conv_layer_filters(self):
    """
      Sorties : Retourne les filtres de la première couche de convolution.
    """
    return self.conv1.weight.clone().cpu().detach().numpy()

  def get_last_conv_layer_filters(self):
    """
      Sorties : Retourne les filtres de la dernière couche de convolution.
    """
    return list(self.layer4.modules())[1].conv2.weight.clone().cpu().detach().numpy()

  def forward(self, x):
    """
      x: Tensor
        Entrée du réseau.

      Sorties : Renvoie la sortie de la passe forward du réseau.
    """
    x = self.activation(self.bn1(self.conv1(x)))
    x = self.layer1(x)
    x = self.layer2(x)
    x = self.layer3(x)
    x = self.layer4(x)
    x = self.avgpool(x)
    x = x.view(x.size(0), -1)
    x = self.linear(x)
    return x

def get_cifar10():  
  transform = transforms.Compose([
      transforms.ToTensor()
  ])

  train_dataset = torchvision.datasets.CIFAR10(
      root='./data', train=True, download=True, transform=transform)
  train_loader = torch.utils.data.DataLoader(
      train_dataset, batch_size=128, shuffle=True, num_workers=2)

  val_dataset = torchvision.datasets.CIFAR10(
      root='./data', train=False, download=True, transform=transform)
  val_loader = torch.utils.data.DataLoader(
      val_dataset, batch_size=128, shuffle=False, num_workers=2)
  
  return train_loader, val_loader

def train_loop(epoch, model, train_loader, criterion, optimizer):
  """
    epoch: int
      Numéro de l'époque d'apprentissage actuelle (à partir de 0).
    model: ResNet18
      Le modèle à entraîner, qui est une instance de la classe ResNet18.
    train_loader: DataLoader
      Training dataloader.
    criterion: Module
      Un objet Module qui évalue la perte de l'entropie croisée.
    optimizer: Optimizer
      Un objet Optimizer pour l'optimiseur Adam.

    Outputs: Retourne la moyenne de train_acc et train_loss pour l'époque actuelle.
  """
  train_acc = 0.
  train_loss = 0.
  model.train()
  for i, (images, labels) in enumerate(train_loader):
    optimizer.zero_grad()
    outputs = model(images)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
    train_acc += (outputs.max(1)[1] == labels).sum().item()
  train_acc, train_loss =  train_acc / len(train_loader.dataset), train_loss / len(train_loader)
  print(f"Epoch: {epoch} | Train Acc: {train_acc:.6f} | Train Loss: {train_loss:.6f}")
  return train_acc, train_loss

def valid_loop(epoch, model, val_loader, criterion):
  """
    epoch: int
      Numéro de l'époque d'apprentissage actuelle (à partir de 0).
    model: ResNet18
      Le modèle à entraîner, qui est une instance de la classe ResNet18.
    val_loader: DataLoader
      The validation dataloader.
    criterion: Module
      Un objet Module qui évalue la perte de l'entropie croisée.

    Outputs: Retourne la moyenne de val_acc et val_loss pour l'époque actuelle.
  """
  val_acc = 0.
  val_loss = 0.
  model.eval()
  with torch.no_grad():
    for i, (images, labels) in enumerate(val_loader):
      outputs = model(images)
      loss = criterion(outputs, labels)
      val_loss += loss.item()
      val_acc += (outputs.max(1)[1] == labels).sum().item()
  val_acc, val_loss = val_acc / len(val_loader.dataset), val_loss / len(val_loader)
  print(f"Epoch: {epoch} | Val Acc: {val_acc:.6f}   | Val Loss: {val_loss:.6f}")
  return val_acc, val_loss

activation_str = "relu"
initialization = "xavier_normal"

if __name__ == "__main__":
  train_accs, train_losses, val_accs, val_losses = [], [], [], []
  n_epochs = 25

  model = ResNet18(
    activation_str=activation_str,
    initialization=initialization
  ).to(device)
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters())

  train_loader, val_loader = get_cifar10()

  for epoch in range(n_epochs):
    # Training
    train_acc, train_loss = train_loop(epoch, model, train_loader, criterion, optimizer)
    train_accs.append(train_acc)
    train_losses.append(train_loss)

    # Validation
    val_acc, val_loss = valid_loop(epoch, model, val_loader, criterion)
    val_accs.append(val_acc)
    val_losses.append(val_loss)

"""### Questions 3.4, 3.5, 3.6, 3.7, 3.8
You may write your own code for these questions below. These will not be autograded and you need not submit code for these, only the report.
"""

# For Q 3.6
if __name__ == "__main__":
  vis_image = None
  for data, labels in val_loader:
    vis_image = data[12].unsqueeze(0)
    break

# You can also upload `vis_image.pkl` from Piazza and use it:
# import pickle
# vis_image = pickle.load(open("vis_image.pkl", "rb")).to(device)
# plt.imshow(vis_image.squeeze().permute(1, 2, 0).cpu().detach().numpy())

# import matplotlib.pyplot as plt
# plt.imshow(vis_image.squeeze().permute(1, 2, 0).cpu().detach().numpy())