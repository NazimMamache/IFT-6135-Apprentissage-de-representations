# -*- coding: utf-8 -*-
"""assignment1_solution.ipynb

Automatically generated by Colaboratory.

# IFT6135-A2022
# Assignment 1: Practical

You must fill in your answers to various questions in this notebook, following which you must export this notebook to a Python file named `solution.py` and submit it on Gradescope.

Only edit the functions specified in the PDF (and wherever marked â€“ `# WRITE CODE HERE`). Do not change definitions or edit the rest of the template, else the autograder will not work.

**Make sure you request a GPU runtime (especially for Question 3)!**
"""

# DO NOT MODIFY!
import matplotlib.pyplot as plt
import numpy as np
import random

# Fix random seed
random.seed(0)
np.random.seed(0)

"""## Question 1: Implementing MLPs with NumPy (30 points)"""

def one_hot(y, n_classes=10):
  """
    y: np.array
      Array of labels (integers from 0 to n_classes-1).
    n_classes: int, default 10
      Number of output classes.

    Outputs: Returns the one-hot encoding of labels.
  """
  return np.eye(n_classes)[y]

class NN(object):
  """
    Implements an MLP.
  """

  def __init__(self,
               dims=(784, 128, 64, 10), # h_0, h_1, h_2, h_3
               activation="relu",       # Activation function
               epsilon=1e-6,            # Correction factor
               lr=0.01,                 # Learning rate
               seed=0                   # Random seed
              ):
    """
      Constructor of the NN class.

      dims: list or tuple or np.array, default (784, 128, 64, 10)
        Values of h_0 (no. of features), h_1 (hidden dim. 1), h_2 (hidden dim. 2), h_3 (no. of output classes).
      activation: string, default "relu"
        Activation function to use.
      epsilon: float or double, default 1e-6
        Correction factor to clip probabilities.
      lr: float or double, default 0.01
        Learning rate for weight updates.
      seed: int, default 0
        Random seed.
    """
    super(NN, self).__init__()

    self.dims = dims
    self.n_hidden = len(dims) - 2
    self.activation_str = activation
    self.epsilon = epsilon
    self.lr = lr
    self.seed = seed

  def initialize_weights(self):
    """
      Results: Initializes the weights of the MLP from uniform(-1/sqrt(h_0), 1/sqrt(h_0)) and the biases to zeros.
    """
    if self.seed is not None:
      np.random.seed(self.seed)

    self.weights = {}
    # self.weights is a dictionary with keys W1, b1, W2, b2, ..., Wm, Bm where m - 1 is the number of hidden layers
    # The keys W1, W2, ..., Wm correspond to weights while b1, b2, ..., bm correspond to biases
    for layer_n in range(1, self.n_hidden + 2):
      self.weights[f"W{layer_n}"] = np.random.uniform(
          low=-1/np.sqrt(self.dims[0]), high=1/np.sqrt(self.dims[0]), size=(self.dims[layer_n - 1], self.dims[layer_n]))
      self.weights[f"b{layer_n}"] = np.zeros((1, self.dims[layer_n]))

  def relu(self, x, grad=False):
    """
      grad: bool, default False
        If True, return the gradient of the activation with respect to the inputs to the function.

      Outputs: Implements the ReLU activation function or its gradient.
    """
    if grad:
      return 1 * (x > 0)
    return x * (x > 0)

  def sigmoid(self, x, grad=False):
    """
      grad: bool, default False
        If True, return the gradient of the activation with respect to the inputs to the function.

      Outputs: Implements the Sigmoid activation function or its gradient.
    """
    sigmoid = np.where(x >= 0, 1 / (1 + np.exp(-x)), np.exp(x) / (1 + np.exp(x))) # numerically stable
    if grad:
      return sigmoid * (1 - sigmoid)
    return sigmoid

  def tanh(self, x, grad=False):
    """
      grad: bool, default False
        If True, return the gradient of the activation with respect to the inputs to the function.

      Outputs: Implements the tanh activation function or its gradient.
    """
    tanh = np.tanh(x)
    if grad:
      return 1 - tanh ** 2
    return tanh

  def activation(self, x, grad=False):
    """
      grad: bool, default False
        If True, return the gradient of the activation with respect to the inputs to the function.

      Outputs: Returns the value of the activation or the gradient.
    """
    if self.activation_str == "relu":
      return self.relu(x, grad=grad)
    elif self.activation_str == "sigmoid":
      return self.sigmoid(x, grad=grad)
    elif self.activation_str == "tanh":
      return self.tanh(x, grad=grad)
    else:
      raise Exception("Invalid activation")

  def softmax(self, x):
    """
      x: np.array
        Inputs to calculate softmax over.

      Outputs: Implements the softmax function, returns the array containing softmax(x).
    """
    # Remember that softmax(x-C) = softmax(x) when C is a constant.
    if len(x.shape) == 1:
      x = x - x.max()
      e_x = np.exp(x)
      return e_x / np.sum(e_x)
    x = x - x.max(axis=1)[:, np.newaxis]
    e_x = np.exp(x)
    return e_x / np.sum(e_x, axis=1)[:, np.newaxis]

  def forward(self, x):
    """
      x: np.array
        Inputs to the MLP. Note that x may contain multiple input examples, not just one example.

      Outputs: Implements the forward pass, returns cache as described below.
    """
    cache = {"Z0": x}
    # cache is a dictionary with keys Z0, A1, Z1, ..., Am, Zm where m - 1 is the number of hidden layers
    # Z0 just contains the inputs x to the network
    # Ai corresponds to the preactivation at layer i, Zi corresponds to the activation at layer i
    for layer_n in range(1, self.n_hidden + 2):
      cache[f"A{layer_n}"] = self.weights[f"b{layer_n}"] + np.matmul(cache[f"Z{layer_n-1}"], self.weights[f"W{layer_n}"])
      if layer_n == self.n_hidden + 1:
        cache[f"Z{layer_n}"] = self.softmax(cache[f"A{layer_n}"])
      else:
        cache[f"Z{layer_n}"] = self.activation(cache[f"A{layer_n}"])
    return cache

  def loss(self, prediction, labels):
    """
      prediction: np.array
        Predicted probabilities for each class for inputs.
      labels: np.array
        True labels corresponding to the inputs (assume they are one-hot encoded).

      Outputs: Returns the crossentropy loss (take the mean over number of inputs).
    """
    prediction[np.where(prediction < self.epsilon)] = self.epsilon
    prediction[np.where(prediction > 1 - self.epsilon)] = 1 - self.epsilon
    entropy_vals = -(labels * np.log(prediction))
    loss = np.sum(entropy_vals) / labels.shape[0]
    return loss

  def backward(self, cache, labels):
    """
      cache: np.array
        Results of the forward pass.
      labels: np.array
        True labels corresponding to the inputs in cache.

      Outputs: Implements the backward pass, returns grads as described below.
    """
    output = cache[f"Z{self.n_hidden + 1}"]
    grads = {}

    # grads is a dictionary with keys dAm, dWm, dbm, dZ(m-1), dA(m-1), ..., dW1, db1
    # Remember to account for the number of input examples!
    grads[f"dA{self.n_hidden + 1}"] = output - labels
    for layer_n in range(self.n_hidden + 1, 0, -1):
      grads[f"dW{layer_n}"] = np.matmul(cache[f"Z{layer_n - 1}"].T, grads[f"dA{layer_n}"]) / cache[f"Z{layer_n - 1}"].shape[0]
      grads[f"db{layer_n}"] = np.sum(grads[f"dA{layer_n}"], axis=0, keepdims=True) / grads[f"dA{layer_n}"].shape[0]
      if layer_n > 1:
        grads[f"dZ{layer_n - 1}"] = np.matmul(self.weights[f"W{layer_n}"], grads[f"dA{layer_n}"].T).T
        grads[f"dA{layer_n - 1}"] = grads[f"dZ{layer_n - 1}"] * self.activation(cache[f"A{layer_n - 1}"], grad=True)
    return grads

  def update(self, grads):
    """
      grads: np.dictionary
        Gradients obtained from the backward pass.

      Results: Updates the network's weights and biases.
    """
    for layer in range(1, self.n_hidden + 2):
      self.weights[f"W{layer}"] -= self.lr * grads[f"dW{layer}"]
      self.weights[f"b{layer}"] -= self.lr * grads[f"db{layer}"]

"""## Question 2: Implementing CNN layers with NumPy (20 points)
Note: You may assume that there are no biases, no input padding (valid convolution) and also that convolution here refers to cross-correlation, i.e., no kernel flipping when convolving the inputs.
"""

class Convolution2dLayer(object):
  """
    Implements a 2D convolution layer.
  """

  def __init__(self, filter_size=3, stride=1, n_units=64, seed=0):
    """
      Constructor of the Convolution2dLayer class.

      Note: We assume that the input images have only a single channel.

      filter_size: int, default 3
        Filter size to use for convolution. We assume equal height and width.
      stride: int, default 1
        Stride for convolution.
      n_units: int, default 64
        Number of output channels, i.e., number of filters in the layer.
      seed: int, default 0
        Random seed.
    """
    super(Convolution2dLayer, self).__init__()

    self.filter_size = filter_size
    self.stride = stride
    self.n_units = n_units
    self.seed = seed

  def initialize_weights(self):     
    """
      Results: Initializes the weights of the CNN from uniform(0, 1) and the biases to zeros.
    """   
    if self.seed is not None:
      np.random.seed(self.seed)

    # self.weights is an np.array of shape (n_units, filter_size, filter_size)
    # We do not consider biases in this convolution layer implementation
    self.weights = np.random.uniform(low=0, high=1, size=(self.n_units, self.filter_size, self.filter_size))

  def forward(self, x):
    """
      x: np.array
        Inputs to convolve. This may contain multiple input examples, not just one.
        Note: We assume that the input images have only a single channel.

      Outputs: Inputs and the result of the convolution operation on the inputs stored in cache.

      Note: You need not flip the kernel! You may just implement cross-correlation.
    """
    cache = {}

    # cache is a dictionary where cache["x"] stores the inputs and cache["out"] stores the outputs of the layer
    cache["x"] = x
    output_hw = int((x.shape[2] - self.filter_size) / self.stride + 1)
    cache["out"] = np.zeros((x.shape[0], self.weights.shape[0], output_hw, output_hw))
    for h in range(output_hw):
      for w in range(output_hw):
        cache["out"][:,:,h,w] = x[:, 0, h:h+self.filter_size, w:w+self.filter_size].reshape(-1, self.filter_size ** 2) @ self.weights.reshape(-1, self.filter_size ** 2).T
    return cache

  def backward(self, cache, grad_output):
    """
      cache: dictionary
        Contains the inputs and the result of the convolution operation applied on them.
      grad_output: np.array
        Gradient of the loss with respect to the outputs of the convolution layer.

      Outputs: Gradient of the loss with respect to the parameters of the convolution layer.
    """
    # grads is an np.array containing the gradient of the loss with respect to the parameters in the convolution layer
    # Remember to account for the number of input examples!
    # WRITE CODE HERE
    x = cache["x"]
    grads = np.zeros(self.weights.shape)
    k = grad_output.shape[2]
    for i in range(self.n_units):
      for h in range(self.filter_size):
        for w in range(self.filter_size):
          grads[i, h, w] = (grad_output[:, i].squeeze() * x.squeeze()[:, h:h+grad_output.shape[2], w:w+grad_output.shape[3]]).sum()
    return grads


class MaxPooling2dLayer(object):
  """
    Implements a 2D max-pooling layer.
  """

  def __init__(self, filter_size=2):
    """
      Constructor of the MaxPooling2dLayer class.

      filter_size: int, default 2
        Filter size to use for max-pooling. We assume equal height and width, and stride = height = width.
    """
    super(MaxPooling2dLayer, self).__init__()
  
    self.filter_size = filter_size
    self.stride = filter_size

  def forward(self, x):
    """
      x: np.array
        Inputs to compute max-pooling for. This may contain multiple input examples, not just one.
        Note: The input dimensions to max-pooling are the output dimensions of the convolution!

      Outputs: Inputs and the result of the max-pooling operation on the inputs stored in cache.
    """
    cache = {}

    # cache is a dictionary where cache["x"] stores the inputs and cache["out"] stores the outputs of the layer
    cache["x"] = x
    output_hw = int(x.shape[2] / self.stride)
    cache["out"] = np.zeros((x.shape[0], x.shape[1], output_hw, output_hw))
    for h in range(output_hw):
      for w in range(output_hw):
        cache["out"][:,:,h,w] = x[:, :, h*self.filter_size:(h+1)*self.filter_size, w*self.filter_size:(w+1)*self.filter_size].max(axis=(2, 3))
    return cache

  def backward(self, cache, grad_output):
    """
      cache: dictionary
        Contains the inputs and the result of the max-pooling operation applied on them.
      grad_output: np.array
        Gradient of the loss with respect to the outputs of the max-pooling layer.

      Outputs: Gradient of the loss with respect to the inputs to the max-pooling layer.
    """
    # grads is an np.array containing the gradient of the loss with respect to the inputs to the max-pooling layer
    # Remember to account for the number of input examples!
    x = cache["x"]
    grads = np.zeros(x.shape)
    for h in range(cache["out"].shape[2]):
      for w in range(cache["out"].shape[3]):
        max_vals = x[:, :, h*self.filter_size:(h+1)*self.filter_size, w*self.filter_size:(w+1)*self.filter_size].max(axis=(2, 3), keepdims=True)
        max_mask = (x[:, :, h*self.filter_size:(h+1)*self.filter_size, w*self.filter_size:(w+1)*self.filter_size] == max_vals).astype(int)
        grads[:, :, h*self.filter_size:(h+1)*self.filter_size, w*self.filter_size:(w+1)*self.filter_size] = max_mask * grad_output[:, :, [[h]], [[w]]]
    return grads

"""## Question 3: Implementing a CNN and comparison with MLPs using PyTorch (50 points)"""

# DO NOT MODIFY!
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.init as init
import torch.optim as optim
import torchvision
import torchvision.transforms as transforms
device = "cuda" if torch.cuda.is_available() else "cpu"

# Fix random seed
torch.manual_seed(0)
torch.cuda.manual_seed_all(0)
torch.cuda.manual_seed(0)
torch.backends.cudnn.deterministic = True
torch.backends.cudnn.benchmark = False

class ResidualBlock(nn.Module):
  """This class implements the Residual Block used in ResNet-18."""

  def __init__(self, in_channels, channels, conv_stride=1, activation_str="relu", initialization="xavier_normal"):
    """
      Constructor for the ResidualBlock class.

      in_channels: int
        Number of channels in the input to the block.
      channels: int
        Number of output channels for the block, i.e., number of filters.
      conv_stride: int, default 1
        Stride of the first convolution layer and downsampling convolution (if required).
      activation_str: string, default "relu"
        Activation function to use.
      initialization: string, default "xavier_normal"
        Initialization for convolution layer weights.
    """
    super(ResidualBlock, self).__init__()

    self.in_channels = in_channels
    self.channels = channels
    self.conv_stride = conv_stride
    self.activation_str = activation_str
    self.initialization = initialization

    # Define these members by replacing `None` with the correct definitions
    self.conv1 = nn.Conv2d(in_channels, channels, kernel_size=3, stride=conv_stride, padding=1, bias=False)
    self.bn1 = nn.BatchNorm2d(channels)
    self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, stride=1, padding=1, bias=False)
    self.bn2 = nn.BatchNorm2d(channels)

    self.residual_connection = self.residual(in_channels, channels, conv_stride)

    # Initialize weights for conv1 and conv2
    if initialization == "xavier_normal":
      init.xavier_normal_(self.conv1.weight)
      init.xavier_normal_(self.conv2.weight)
    elif initialization == "xavier_uniform":
      init.xavier_uniform_(self.conv1.weight)
      init.xavier_uniform_(self.conv2.weight)
    elif initialization == "kaiming_normal":
      init.kaiming_normal_(self.conv1.weight)
      init.kaiming_normal_(self.conv2.weight)
    else:
      raise Exception("Invalid initialization")

  def activation(self, input):
    """
      input: Tensor
        Input on which the activation is applied.

      Output: Result of activation function applied on input.
        E.g. if self.activation_str is "relu", return relu(input).
    """
    if self.activation_str == "relu":
      return F.relu(input)
    elif self.activation_str == "tanh":
      return torch.tanh(input)
    else:
      raise Exception("Invalid activation")
    return 0

  def residual(self, in_channels, channels, conv_stride=1):
    """
      in_channels: int
        Number of input channels in the input to the block.
      channels: int
        Number of output channels for the block, i.e., number of filters.
      conv_stride: int, default 1
        Stride to use for downsampling 1x1 convolution.

      Output: Returns an nn.Sequential object which computes the identity function of the input if stride is 1
              and the number of input channels equals the number of output channels. Otherwise, it returns an
              nn.Sequential object that downsamples its input using a 1x1-conv of the stride specified and
              followed by a BatchNorm2d.
    """
    layers = []
    if conv_stride != 1 or in_channels != channels:
      layers.append(nn.Conv2d(in_channels, channels, kernel_size=1, stride=conv_stride, bias=False))
      layers.append(nn.BatchNorm2d(channels))
    return nn.Sequential(*layers)

  def forward(self, x):
    """
      x: Tensor
        Input to the block.

      Outputs: Returns the output of the forward pass of the block.
    """
    out = self.activation(self.bn1(self.conv1(x)))
    out = self.bn2(self.conv2(out))
    out += self.residual_connection(x)
    out = self.activation(out)
    return out

class ResNet18(nn.Module):
  """This class implements the ResNet-18 architecture from its components."""

  def __init__(self, activation_str="relu", initialization="xavier_normal"):
    """
      Constructor for the ResNet18 class.

      activation_str: string, default "relu"
        Activation function to use.
      initialization: string, default "xavier_normal"
        Weight initialization to use.
    """
    super(ResNet18, self).__init__()

    self.n_classes = 10
    self.activation_str = activation_str
    self.initialization = initialization

    # Define these members by replacing `None` with the correct definitions
    self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    self.bn1 = nn.BatchNorm2d(64)
    self.layer1 = self._create_layer(64, 64, 1)
    self.layer2 = self._create_layer(64, 128, 2)
    self.layer3 = self._create_layer(128, 256, 2)
    self.layer4 = self._create_layer(256, 512, 2)
    self.avgpool = nn.AdaptiveAvgPool2d((1, 1))
    self.linear = nn.Linear(512, self.n_classes)
  
  def activation(self, input):
    """
      input: Tensor
        Input on which the activation is applied.

      Output: Result of activation function applied on input.
        E.g. if self.activation_str is "relu", return relu(input).
    """
    if self.activation_str == "relu":
      return F.relu(input)
    elif self.activation_str == "tanh":
      return F.tanh(input)
    else:
      raise Exception("Invalid activation")

  def _create_layer(self, in_channels, channels, conv_stride=1):
    """
      in_channels: int
        Number of input channels present in the input to the layer.
      out_channels: int
        Number of output channels for the layer, i.e., the number of filters.
      conv_stride: int, default 1
        Stride of the first convolution layer in the block and the downsampling convolution (if required).

      Outputs: Returns an nn.Sequential object giving a "layer" of the ResNet, consisting of 2 blocks each.
    """
    # Modify the following statement to return an nn.Sequential object containing 2 ResidualBlocks.
    # You must make sure that the appropriate channels and conv_stride are provided.
    return nn.Sequential(
      ResidualBlock(in_channels, channels, conv_stride, self.activation_str, self.initialization),
      ResidualBlock(channels, channels, 1, self.activation_str, self.initialization)
    )

  def get_first_conv_layer_filters(self):
    """
      Outputs: Returns the filters in the first convolution layer.
    """
    return self.conv1.weight

  def get_last_conv_layer_filters(self):
    """
      Outputs: Returns the filters in the last convolution layer.
    """
    return self.layer4[1].conv2.weight

  def forward(self, x):
    """
      x: Tensor
        Input to the network.

      Outputs: Returns the output of the forward pass of the network.
    """
    out = self.activation(self.bn1(self.conv1(x)))
    out = self.layer1(out)
    out = self.layer2(out)
    out = self.layer3(out)
    out = self.layer4(out)
    out = self.avgpool(out)
    out = self.linear(out.view(out.size(0), -1))
    return out

def get_cifar10():  
  transform = transforms.Compose([
      transforms.ToTensor()
  ])

  train_dataset = torchvision.datasets.CIFAR10(
      root='./data', train=True, download=True, transform=transform)
  train_loader = torch.utils.data.DataLoader(
      train_dataset, batch_size=128, shuffle=True, num_workers=2)

  val_dataset = torchvision.datasets.CIFAR10(
      root='./data', train=False, download=True, transform=transform)
  val_loader = torch.utils.data.DataLoader(
      val_dataset, batch_size=128, shuffle=False, num_workers=2)
  
  return train_loader, val_loader

def train_loop(epoch, model, train_loader, criterion, optimizer):
  """
    epoch: int
      Number of the current training epoch (starting from 0).
    model: ResNet18
      The model to train, which is an instance of the ResNet18 class.
    train_loader: DataLoader
      The training dataloader.
    criterion: Module
      A Module object that evaluates the crossentropy loss.
    optimizer: Optimizer
      An Optimizer object for the Adam optimizer.

    Outputs: Returns average train_acc and train_loss for the current epoch.
  """
  train_acc = 0.
  train_loss = 0.

  model.train()
  for data, labels in train_loader:
    optimizer.zero_grad()
    data, labels = data.to(device), labels.to(device)
    predictions = model(data)
    loss = criterion(predictions, labels)
    loss.backward()
    optimizer.step()
    train_loss += loss.item()
    train_acc += (predictions.argmax(axis=1) == labels).float().mean()

  train_acc /= len(train_loader)
  train_loss /= len(train_loader)

  print(f"Epoch: {epoch} | Train Acc: {train_acc:.6f} | Train Loss: {train_loss:.6f}")
  return train_acc, train_loss

def valid_loop(epoch, model, val_loader, criterion):
  """
    epoch: int
      Number of the current epoch (starting from 0).
    model: ResNet18
      The model to train, which is an instance of the ResNet18 class.
    val_loader: DataLoader
      The validation dataloader.
    criterion: Module
      A Module object that evaluates the crossentropy loss.

    Outputs: Returns val_acc and val_loss for the current epoch.
  """
  val_acc = 0.
  val_loss = 0.

  model.eval()
  with torch.no_grad():
    for data, labels in val_loader:
      data, labels = data.to(device), labels.to(device)
      predictions = model(data)
      loss = criterion(predictions, labels)
      val_loss += loss.item()
      val_acc += (predictions.argmax(axis=1) == labels).float().mean()

    val_loss /= len(val_loader)
    val_acc /= len(val_loader)

  print(f"Epoch: {epoch} | Val Acc: {val_acc:.6f}   | Val Loss: {val_loss:.6f}")
  return val_acc, val_loss

activation_str = "relu"
initialization = "xavier_normal"

if __name__ == "__main__":
  train_accs, train_losses, val_accs, val_losses = [], [], [], []
  n_epochs = 25

  model = ResNet18(
    activation_str=activation_str,
    initialization=initialization
  ).to(device)
  criterion = nn.CrossEntropyLoss()
  optimizer = optim.Adam(model.parameters())

  train_loader, val_loader = get_cifar10()

  for epoch in range(n_epochs):
    # Training
    train_acc, train_loss = train_loop(epoch, model, train_loader, criterion, optimizer)
    train_accs.append(train_acc)
    train_losses.append(train_loss)

    # Validation
    val_acc, val_loss = valid_loop(epoch, model, val_loader, criterion)
    val_accs.append(val_acc)
    val_losses.append(val_loss)

"""### Questions 3.4, 3.5, 3.6, 3.7, 3.8
You may write your own code for these questions below. These will not be autograded and you need not submit code for these, only the report.
"""

