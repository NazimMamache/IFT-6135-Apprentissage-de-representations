{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "d5BRk4fZr7wf"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FKcid9m6r7wk"
      },
      "source": [
        "# IFT6135-A2022<br>\n",
        "# Assignment 2: Practical<br>\n",
        "<br>\n",
        "You must fill in your answers to various questions in this notebook, following which you must export this notebook to a Python file named `solution.py` and submit it on Gradescope.<br>\n",
        "<br>\n",
        "Only edit the functions specified in the PDF (and wherever marked – `# WRITE CODE HERE`). Do not change definitions or edit the rest of the template, else the autograder will not work.<br>\n",
        "<br>\n",
        "**Make sure you request a GPU runtime!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wK64qfM7r7wn"
      },
      "source": [
        "DO NOT MODIFY!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nvidia-ml-py3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXy5JAC3k_DE",
        "outputId": "dc859df9-dcc8-41e4-be92-50fe368ebca5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nvidia-ml-py3\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "Building wheels for collected packages: nvidia-ml-py3\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19191 sha256=185be562663216bee060a12d513f41d83fa95cc5bcfb87aab2856b0c021aa2a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/df/99/da/c34f202dc8fd1dffd35e0ecf1a7d7f8374ca05fbcbaf974b83\n",
            "Successfully built nvidia-ml-py3\n",
            "Installing collected packages: nvidia-ml-py3\n",
            "Successfully installed nvidia-ml-py3-7.352.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XFkQNIupr7wn"
      },
      "outputs": [],
      "source": [
        "from dis import disassemble\n",
        "import math\n",
        "import json\n",
        "import time\n",
        "import nvidia_smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VrRH7oQNr7wo"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "woWUlMOnr7wp"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PEJou5lFr7wq"
      },
      "outputs": [],
      "source": [
        "def fix_experiment_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AxtnNYu8r7wq"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdbG8CYGr7wr"
      },
      "source": [
        "#### Data pre-processing<br>\n",
        "<br>\n",
        "Run the cells below by clicking the Run/Play button, but do not modify the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "J6S4tgPEr7ws"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "import io\n",
        "import os\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fG-k1-rBr7wt",
        "outputId": "f1655362-82cb-4d35-fffe-bb55ee1e10db"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "os.system(\"python3 -m spacy download en_core_web_sm\")\n",
        "os.system(\"python3 -m spacy download fr_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AAZG9X5Tr7wu"
      },
      "outputs": [],
      "source": [
        "en_tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", \"en_core_web_sm\")\n",
        "fr_tokenizer = torchtext.data.utils.get_tokenizer(\"spacy\", \"fr_core_news_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOEhcwixr7wv",
        "outputId": "8eb915c3-0cb1-4841-8fee-0e90d41d1e17"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 569k/569k [00:00<00:00, 36.1MB/s]\n",
            "100%|██████████| 604k/604k [00:00<00:00, 43.6MB/s]\n",
            "100%|██████████| 21.6k/21.6k [00:00<00:00, 8.65MB/s]\n",
            "100%|██████████| 23.0k/23.0k [00:00<00:00, 6.75MB/s]\n",
            "100%|██████████| 21.1k/21.1k [00:00<00:00, 16.8MB/s]\n",
            "100%|██████████| 22.3k/22.3k [00:00<00:00, 5.16MB/s]\n"
          ]
        }
      ],
      "source": [
        "base_url = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "train_urls = [\"train.en.gz\", \"train.fr.gz\"]\n",
        "val_urls = [\"val.en.gz\", \"val.fr.gz\"]\n",
        "test_urls = [\"test_2016_flickr.en.gz\", \"test_2016_flickr.fr.gz\"]\n",
        "train_paths = [extract_archive(download_from_url(base_url + url))[0] for url in train_urls]\n",
        "val_paths = [extract_archive(download_from_url(base_url + url))[0] for url in val_urls]\n",
        "test_paths = [extract_archive(download_from_url(base_url + url))[0] for url in test_urls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "axkZ300Fr7ww"
      },
      "outputs": [],
      "source": [
        "def generate_vocabulary(path, tokenizer):\n",
        "    counter = Counter()\n",
        "    with io.open(path, encoding=\"utf8\") as f:\n",
        "        for item in f:\n",
        "            counter.update(tokenizer(item))\n",
        "    vocabulary = torchtext.vocab.vocab(counter, specials=[\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"])\n",
        "    vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
        "    return vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "GpnRDGi6r7wx"
      },
      "outputs": [],
      "source": [
        "en_vocabulary = generate_vocabulary(train_paths[0], en_tokenizer)\n",
        "fr_vocabulary = generate_vocabulary(train_paths[1], fr_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "In1n0O6Qr7wy"
      },
      "outputs": [],
      "source": [
        "def get_processed_data(paths):\n",
        "    en_file_io = iter(io.open(paths[0], encoding=\"utf8\"))\n",
        "    fr_file_io = iter(io.open(paths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for en_item, fr_item in zip(en_file_io, fr_file_io):\n",
        "        en_processed = torch.tensor([en_vocabulary[token] for token in en_tokenizer(en_item)], dtype=torch.long)\n",
        "        fr_processed = torch.tensor([fr_vocabulary[token] for token in fr_tokenizer(fr_item)], dtype=torch.long)\n",
        "        data.append((en_processed, fr_processed))\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "k-XcK-ANr7wz"
      },
      "outputs": [],
      "source": [
        "train_data = get_processed_data(train_paths)\n",
        "val_data = get_processed_data(val_paths)\n",
        "test_data = get_processed_data(test_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkaV8ORAr7w0"
      },
      "source": [
        "### GRU Encoder-Decoder Model for Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JVj6PS6r7w1"
      },
      "source": [
        "#### Dataloader<br>\n",
        "<br>\n",
        "Running this cell will create `train_loader`, `val_loader` and `test_loader` for training, validation and testing respectively. The batch size is 128 and all sequences are of length 60 – shorter sequences are padded to this length. For this question, the data is provided in the shape `[sequence_length, batch_size]`. You need not modify this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iuKJyWMcr7w2"
      },
      "outputs": [],
      "source": [
        "special_idx = {\n",
        "    \"bos\": en_vocabulary[\"<bos>\"],\n",
        "    \"pad\": en_vocabulary[\"<pad>\"],\n",
        "    \"eos\": en_vocabulary[\"<eos>\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "6LYybmrRr7w2"
      },
      "outputs": [],
      "source": [
        "max_len = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "FQ82R0N5r7w3"
      },
      "outputs": [],
      "source": [
        "def get_batch(data):\n",
        "    en_batch_, fr_batch_ = [], []\n",
        "    for en_item, fr_item in data:\n",
        "        en_batch_.append(torch.cat([torch.tensor([special_idx[\"bos\"]]), en_item, torch.tensor([special_idx[\"eos\"]])], dim=0))\n",
        "        fr_batch_.append(torch.cat([torch.tensor([special_idx[\"bos\"]]), fr_item, torch.tensor([special_idx[\"eos\"]])], dim=0))\n",
        "    en_batch_ = nn.utils.rnn.pad_sequence(en_batch_, padding_value=special_idx[\"pad\"])\n",
        "    fr_batch_ = nn.utils.rnn.pad_sequence(fr_batch_, padding_value=special_idx[\"pad\"])\n",
        "    en_batch = torch.full((max_len, en_batch_.shape[1]), special_idx[\"pad\"])\n",
        "    en_batch[:en_batch_.shape[0], :] = en_batch_\n",
        "    fr_batch = torch.full((max_len, fr_batch_.shape[1]), special_idx[\"pad\"])\n",
        "    fr_batch[:fr_batch_.shape[0], :] = fr_batch_\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3l1OAGaDr7w5"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=get_batch)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=get_batch)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=get_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74HJTxtr7w6"
      },
      "source": [
        "#### Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "rAuBV5kOr7w7"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines an encoder model containing a GRU.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary_size=10838, # Size of source vocabulary, given by len(en_vocabulary)\n",
        "        embedding_size=32,     # Size of nn.Embedding\n",
        "        hidden_size=64,        # Hidden size for nn.GRU\n",
        "        num_layers=1,          # Number of layers in the nn.GRU\n",
        "        use_dropout=False,     # Whether or not to use Dropout\n",
        "        p_dropout=0.1          # Dropout probability\n",
        "    ):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.use_dropout = use_dropout\n",
        "        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_size, padding_idx=special_idx[\"pad\"])\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(p_dropout)\n",
        "        self.gru =  nn.GRU(self.embedding_size, self.hidden_size, self.num_layers, batch_first=False)\n",
        "    def forward(self, x):\n",
        "        # WRITE CODE HERE\n",
        "        # x -> embedding -> dropout (if use_dropout) -> gru -> return output and final hidden\n",
        "        x = self.embedding(x)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        output, hidden = self.gru(x)\n",
        "        return output, hidden\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tUu9-b2yr7w9"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a decoder model containing a GRU.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary_size=11510, # Size of target vocabulary, given by len(fr_vocabulary)\n",
        "        embedding_size=32,     # Size of nn.Embedding\n",
        "        hidden_size=64,        # Hidden size for nn.GRU\n",
        "        num_layers=1,          # Number of layers in the nn.GRU\n",
        "        use_dropout=False,     # Whether or not to use Dropout\n",
        "        p_dropout=0.1          # Dropout probability\n",
        "    ):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_size = embedding_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.use_dropout = use_dropout\n",
        "        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(p_dropout)\n",
        "        self.gru = nn.GRU(self.embedding_size, self.hidden_size, self.num_layers, batch_first=False)\n",
        "        self.fc = nn.Linear(self.hidden_size, self.vocabulary_size)\n",
        "    \n",
        "    def forward(self, x, hidden):\n",
        "        x = x.unsqueeze(0)\n",
        "\n",
        "        # WRITE CODE HERE\n",
        "        # x -> embedding -> dropout (if use_dropout) -> gru -> output and final hidden\n",
        "        # output -> fc -> out\n",
        "        # return out and final hidden\n",
        "        x = self.embedding(x)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        output, hidden = self.gru(x, hidden)\n",
        "        out = self.fc(output.squeeze(0))\n",
        "        return out, hidden"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "VMkDAjZVr7w-"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines an encoder-decoder (sequence-to-sequence) model for machine translation.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        encoder,\n",
        "        decoder,\n",
        "        strategy=\"greedy\"\n",
        "    ):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.strategy = strategy\n",
        "    def forward(self, source, target):\n",
        "        batch_size = source.shape[1]\n",
        "        max_length = target.shape[0]\n",
        "        target_vocabulary_size = self.decoder.vocabulary_size\n",
        "        outputs = torch.zeros(max_length, batch_size, target_vocabulary_size).to(device)\n",
        "        sentences = torch.zeros(max_length, batch_size).to(device)\n",
        "\n",
        "        # WRITE CODE HERE\n",
        "        # Get the encoder outputs and hidden state for the source sentences\n",
        "        encoder_out, hidden = self.encoder(source)\n",
        "\n",
        "        # The first input token passed to the decoder is <bos>\n",
        "        out = target[0, :]\n",
        "        if self.strategy == \"greedy\":\n",
        "            for idx in range(1, max_length):\n",
        "                # WRITE CODE HERE\n",
        "                # Get the decoder outputs and hidden state for the input token, encoder outputs and hidden state\n",
        "                out, hidden = self.decoder(out, hidden)\n",
        "                outputs[idx] = out\n",
        "                # WRITE CODE HERE\n",
        "                # Reassign out with the greedy choice (argmax) for each batch\n",
        "                # (batch_size, vocabulary_size) -> (batch_size)\n",
        "                out = out.argmax(1)\n",
        "                sentences[idx] = out\n",
        "        elif self.strategy == \"random\":\n",
        "            temperature = 0.5\n",
        "            for idx in range(1, max_length):\n",
        "                # WRITE CODE HERE\n",
        "                # Get the decoder outputs and hidden state for the input token, encoder outputs and hidden state\n",
        "                out, hidden = self.decoder(out, hidden)\n",
        "                outputs[idx] = out\n",
        "                # WRITE CODE HERE\n",
        "                # Reassign out with the randomly sampled choice for each batch\n",
        "                # p(x_i | x_{1:i-1}) =  exp(o_i / temperature) / sum(exp(o / temperature))\n",
        "                # (batch_size, vocabulary_size) -> (batch_size)\n",
        "                temperature = 0.5\n",
        "                out = torch.multinomial(torch.softmax(out / temperature, dim=1), 1).squeeze(1)\n",
        "                sentences[idx] = out\n",
        "        else:\n",
        "            raise Exception(\"Invalid decoding strategy!\")\n",
        "        return outputs, sentences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlsJR3oEr7w_"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "hYDUXBLkr7w_"
      },
      "outputs": [],
      "source": [
        "def get_criterion():\n",
        "    # WRITE CODE HERE\n",
        "    # The criterion must compute the cross-entropy loss but ignore special_idx[\"pad\"].\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=special_idx[\"pad\"])\n",
        "    return criterion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "sCan9uCgr7xA"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "    train_loss = 0.\n",
        "    train_ppl = 0.\n",
        "    model.train()\n",
        "    start = time.time()\n",
        "    for source, target in dataloader:\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(source, target)\n",
        "        output_reshape = output[1:].view(-1, output.shape[-1])\n",
        "        target = target[1:].view(-1)\n",
        "        loss = criterion(output_reshape, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    end = time.time()\n",
        "    train_loss /= len(dataloader)\n",
        "    train_ppl = math.exp(train_loss)\n",
        "    train_time = end - start\n",
        "    return train_loss, train_ppl, train_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "qejAPDtzr7xA"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "    val_loss = 0.\n",
        "    val_ppl = 0.\n",
        "    model.eval()\n",
        "    start = time.time()\n",
        "    for source, target in dataloader:\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        output, _ = model(source, target)\n",
        "        output_reshape = output[1:].view(-1, output.shape[-1])\n",
        "        target = target[1:].view(-1)\n",
        "        loss = criterion(output_reshape, target)\n",
        "        val_loss += loss.item()\n",
        "    end = time.time()\n",
        "    val_loss /= len(dataloader)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    val_time = end - start\n",
        "    return val_loss, val_ppl, val_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "W5H7FszIr7xB"
      },
      "outputs": [],
      "source": [
        "def compute_bleu_gru(model, dataloader):\n",
        "    targets = []\n",
        "    translations = []\n",
        "    for source, target in dataloader:\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        _, output = model(source, target)\n",
        "        for i in range(target.shape[1]):\n",
        "            targets.append([[]])\n",
        "            for token in target[1:, i]:\n",
        "                if token in special_idx.values():\n",
        "                    break\n",
        "                targets[-1][0].append(fr_vocabulary.get_itos()[token])\n",
        "            translations.append([])\n",
        "            for token in output[1:, i]:\n",
        "                if token in special_idx.values():\n",
        "                    break\n",
        "                translations[-1].append(fr_vocabulary.get_itos()[token.int().item()])\n",
        "    return torchtext.data.metrics.bleu_score(translations, targets, max_n=1, weights=[1]) * 100,            torchtext.data.metrics.bleu_score(translations, targets, max_n=2, weights=[0.5]*2) * 100,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "qTs98WFrr7xD"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = False\n",
        "strategy = \"greedy\"\n",
        "encoder = Encoder(use_dropout=use_dropout).to(device)\n",
        "decoder = Decoder(use_dropout=use_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, strategy=strategy).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "KuCRhO56r7xF"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\":  [],\n",
        "    \"info_free\":  [],\n",
        "    \"info_used\":  [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "guCke0c1r7xI",
        "outputId": "9e406f63-0350-47f0-8a2b-da2362321ae3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 6.023 | Train PPL: 412.884 | Train Time: 90.452\n",
            "Valid Loss: 5.125 | Valid PPL: 168.192 | Valid Time: 1.370\n",
            "Memory : (92.61% free): 15843721216(total), 14673510400 (free), 1170210816 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 5.062 | Train PPL: 157.861 | Train Time: 87.788\n",
            "Valid Loss: 5.046 | Valid PPL: 155.342 | Valid Time: 1.363\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 5.008 | Train PPL: 149.572 | Train Time: 87.606\n",
            "Valid Loss: 5.023 | Valid PPL: 151.890 | Valid Time: 1.376\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 4.985 | Train PPL: 146.167 | Train Time: 87.653\n",
            "Valid Loss: 5.021 | Valid PPL: 151.536 | Valid Time: 1.365\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 4.971 | Train PPL: 144.183 | Train Time: 87.783\n",
            "Valid Loss: 5.017 | Valid PPL: 151.017 | Valid Time: 1.391\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 4.963 | Train PPL: 142.954 | Train Time: 87.544\n",
            "Valid Loss: 5.018 | Valid PPL: 151.169 | Valid Time: 1.382\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 4.956 | Train PPL: 141.977 | Train Time: 87.591\n",
            "Valid Loss: 5.019 | Valid PPL: 151.247 | Valid Time: 1.357\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 4.950 | Train PPL: 141.228 | Train Time: 87.645\n",
            "Valid Loss: 5.019 | Valid PPL: 151.295 | Valid Time: 1.377\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 4.946 | Train PPL: 140.563 | Train Time: 87.541\n",
            "Valid Loss: 5.020 | Valid PPL: 151.409 | Valid Time: 1.360\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 4.942 | Train PPL: 140.090 | Train Time: 87.522\n",
            "Valid Loss: 5.023 | Valid PPL: 151.813 | Valid Time: 1.383\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "== Test Loss: 5.005 | Test PPL: 149.119 | Test Time: 1.362 ==\n",
            "Test BLEU-1: 24.054333107914104 Test BLEU-2: 14.33895271211818\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu_gru(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"seq2seq-adam-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "L5w-dzeFr7xL"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = True\n",
        "strategy = \"greedy\"\n",
        "encoder = Encoder(use_dropout=use_dropout).to(device)\n",
        "decoder = Decoder(use_dropout=use_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, strategy=strategy).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "pdJ7hu5er7xM"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\":  [],\n",
        "    \"info_free\":  [],\n",
        "    \"info_used\":  [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u5br9VVr7xN",
        "outputId": "b9c05e98-e394-4dd3-9357-3fa81736d7e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 9.308 | Train PPL: 11029.010 | Train Time: 87.944\n",
            "Valid Loss: 9.248 | Valid PPL: 10383.806 | Valid Time: 1.360\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 9.132 | Train PPL: 9245.380 | Train Time: 87.952\n",
            "Valid Loss: 8.715 | Valid PPL: 6090.888 | Valid Time: 1.366\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 8.122 | Train PPL: 3366.543 | Train Time: 87.969\n",
            "Valid Loss: 7.611 | Valid PPL: 2019.696 | Valid Time: 1.376\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 7.565 | Train PPL: 1929.451 | Train Time: 87.947\n",
            "Valid Loss: 7.388 | Valid PPL: 1616.506 | Valid Time: 1.358\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 7.175 | Train PPL: 1306.021 | Train Time: 87.869\n",
            "Valid Loss: 7.023 | Valid PPL: 1122.478 | Valid Time: 1.375\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 6.901 | Train PPL: 993.585 | Train Time: 88.104\n",
            "Valid Loss: 6.798 | Valid PPL: 895.692 | Valid Time: 1.424\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 6.712 | Train PPL: 822.402 | Train Time: 87.973\n",
            "Valid Loss: 6.632 | Valid PPL: 759.335 | Valid Time: 1.393\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 6.580 | Train PPL: 720.492 | Train Time: 87.813\n",
            "Valid Loss: 6.523 | Valid PPL: 680.632 | Valid Time: 1.369\n",
            "Memory : (78.08% free): 15843721216(total), 12370837504 (free), 3472883712 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 6.463 | Train PPL: 641.173 | Train Time: 87.773\n",
            "Valid Loss: 6.396 | Valid PPL: 599.354 | Valid Time: 1.380\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 6.333 | Train PPL: 562.824 | Train Time: 87.870\n",
            "Valid Loss: 6.271 | Valid PPL: 528.978 | Valid Time: 1.382\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "== Test Loss: 6.289 | Test PPL: 538.515 | Test Time: 1.436 ==\n",
            "Test BLEU-1: 3.538982942700386 Test BLEU-2: 0.0\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu_gru(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"seq2seq-sgd-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "zfrcgiJ5r7xO"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = True\n",
        "strategy = \"greedy\"\n",
        "encoder = Encoder(use_dropout=use_dropout).to(device)\n",
        "decoder = Decoder(use_dropout=use_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, strategy=strategy).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "tz31K7OTr7xO"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\":  [],\n",
        "    \"info_free\":  [],\n",
        "    \"info_used\":  [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wH7rxnjr7xP",
        "outputId": "38fb0b93-984f-4923-9153-d637620f1339"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 5.889 | Train PPL: 361.086 | Train Time: 88.060\n",
            "Valid Loss: 5.103 | Valid PPL: 164.547 | Valid Time: 1.393\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 5.048 | Train PPL: 155.746 | Train Time: 87.998\n",
            "Valid Loss: 5.039 | Valid PPL: 154.278 | Valid Time: 1.378\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 5.001 | Train PPL: 148.632 | Train Time: 88.049\n",
            "Valid Loss: 5.022 | Valid PPL: 151.766 | Valid Time: 1.425\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 4.982 | Train PPL: 145.770 | Train Time: 88.048\n",
            "Valid Loss: 5.021 | Valid PPL: 151.617 | Valid Time: 1.392\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 4.970 | Train PPL: 144.010 | Train Time: 88.053\n",
            "Valid Loss: 5.019 | Valid PPL: 151.295 | Valid Time: 1.379\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 4.962 | Train PPL: 142.881 | Train Time: 88.122\n",
            "Valid Loss: 5.021 | Valid PPL: 151.575 | Valid Time: 1.369\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 4.955 | Train PPL: 141.951 | Train Time: 88.076\n",
            "Valid Loss: 5.021 | Valid PPL: 151.544 | Valid Time: 1.364\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 4.950 | Train PPL: 141.221 | Train Time: 88.062\n",
            "Valid Loss: 5.021 | Valid PPL: 151.567 | Valid Time: 1.353\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 4.940 | Train PPL: 139.708 | Train Time: 87.988\n",
            "Valid Loss: 4.999 | Valid PPL: 148.249 | Valid Time: 1.368\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 4.890 | Train PPL: 132.992 | Train Time: 88.132\n",
            "Valid Loss: 4.930 | Valid PPL: 138.386 | Valid Time: 1.353\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "== Test Loss: 4.909 | Test PPL: 135.438 | Test Time: 1.359 ==\n",
            "Test BLEU-1: 27.17851951208979 Test BLEU-2: 16.10931339543667\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu_gru(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"seq2seq-adam-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "kFNx9wAVr7xQ"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = True\n",
        "strategy = \"random\"\n",
        "encoder = Encoder(use_dropout=use_dropout).to(device)\n",
        "decoder = Decoder(use_dropout=use_dropout).to(device)\n",
        "model = Seq2Seq(encoder, decoder, strategy=strategy).to(device)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "Ty0K6_Zxr7xS"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\":  [],\n",
        "    \"info_free\":  [],\n",
        "    \"info_used\":  [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAA-ow2Mr7xT",
        "outputId": "a7fc1f54-5307-4a1d-e107-7f82174bcfa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 5.937 | Train PPL: 378.653 | Train Time: 93.661\n",
            "Valid Loss: 5.160 | Valid PPL: 174.095 | Valid Time: 1.565\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 5.080 | Train PPL: 160.729 | Train Time: 93.626\n",
            "Valid Loss: 5.059 | Valid PPL: 157.414 | Valid Time: 1.549\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 5.023 | Train PPL: 151.885 | Train Time: 93.532\n",
            "Valid Loss: 5.037 | Valid PPL: 154.014 | Valid Time: 1.559\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 5.001 | Train PPL: 148.547 | Train Time: 93.694\n",
            "Valid Loss: 5.034 | Valid PPL: 153.554 | Valid Time: 1.565\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 4.987 | Train PPL: 146.454 | Train Time: 93.546\n",
            "Valid Loss: 5.028 | Valid PPL: 152.635 | Valid Time: 1.565\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 4.916 | Train PPL: 136.403 | Train Time: 93.672\n",
            "Valid Loss: 4.847 | Valid PPL: 127.356 | Valid Time: 1.587\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 4.734 | Train PPL: 113.735 | Train Time: 93.663\n",
            "Valid Loss: 4.762 | Valid PPL: 116.965 | Valid Time: 1.576\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 4.640 | Train PPL: 103.579 | Train Time: 93.631\n",
            "Valid Loss: 4.663 | Valid PPL: 105.962 | Valid Time: 1.572\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 4.569 | Train PPL:  96.457 | Train Time: 93.595\n",
            "Valid Loss: 4.605 | Valid PPL:  99.972 | Valid Time: 1.568\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 4.512 | Train PPL:  91.073 | Train Time: 93.667\n",
            "Valid Loss: 4.561 | Valid PPL:  95.679 | Valid Time: 1.563\n",
            "Memory : (78.07% free): 15843721216(total), 12368740352 (free), 3474980864 (used)\n",
            "== Test Loss: 4.532 | Test PPL:  92.990 | Test Time: 1.560 ==\n",
            "Test BLEU-1: 33.80490007859946 Test BLEU-2: 16.87078362836344\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu_gru(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"seq2seq-adam-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnlZXGk2r7xU"
      },
      "source": [
        "#### Basic Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "DxyCiWeor7xV"
      },
      "outputs": [],
      "source": [
        "class TestSeq2SeqShapes():\n",
        "    def __init__(self):\n",
        "        self.vocabulary_size = 11\n",
        "        self.embedding_size = 13\n",
        "        self.hidden_size = 17\n",
        "        self.num_layers = 1\n",
        "        self.encoder = Encoder(\n",
        "            vocabulary_size=self.vocabulary_size,\n",
        "            embedding_size=self.embedding_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            use_dropout=False,\n",
        "            p_dropout=0.1\n",
        "        )\n",
        "        self.decoder = Decoder(\n",
        "            vocabulary_size=self.vocabulary_size,\n",
        "            embedding_size=self.embedding_size,\n",
        "            hidden_size=self.hidden_size,\n",
        "            num_layers=self.num_layers,\n",
        "            use_dropout=False,\n",
        "            p_dropout=0.1\n",
        "        )\n",
        "        self.model = Seq2Seq(\n",
        "            self.encoder, self.decoder, strategy=\"greedy\"\n",
        "        )\n",
        "    def test_forward_shape(self):\n",
        "        batch_size = 5\n",
        "        sequence_length = 7\n",
        "        tokens = torch.randint(self.vocabulary_size, size=(sequence_length, batch_size))\n",
        "        outputs = self.encoder(tokens)\n",
        "        assert isinstance(outputs, (tuple, list)), (\n",
        "            \"The output of the module must be encoder_outputs, hidden_state.\"\n",
        "        )\n",
        "        assert len(outputs) == 2, (\n",
        "            \"The output of the module must be encoder_outputs, hidden_state.\"\n",
        "        )\n",
        "        out, h = outputs\n",
        "        expected_shape = (1, batch_size, self.hidden_size)\n",
        "        assert isinstance(h, torch.Tensor), (\n",
        "            \"The hidden_state must be a torch.Tensor.\"\n",
        "        )\n",
        "        assert h.shape == expected_shape, (\n",
        "            \"The shape of `hidden_state` is invalid. Got shape {0}\"\n",
        "            \"\\n  Expected shape: {1}\\n Recall that the final hidden_state should have shape \"\n",
        "            \"(1, batch_size, hidden_size)\".format(\n",
        "                tuple(h.shape), expected_shape\n",
        "            )\n",
        "        )\n",
        "        outputs = self.decoder(tokens[0, :], h)\n",
        "        expected_shape = (batch_size, self.vocabulary_size)\n",
        "        assert isinstance(outputs, (tuple, list)), (\n",
        "            \"The output of the module must ba a tuple scores, hidden_state.\"\n",
        "        )\n",
        "        assert len(outputs) == 2, (\n",
        "            \"The output of the module must ba a tuple scores, hidden_state.\"\n",
        "        )\n",
        "        out, h = outputs\n",
        "        assert out.shape == expected_shape, (\n",
        "            \"The shape of decoder output is invalid. Got shape {0}.\"\n",
        "            \"\\n  Expected shape: {1}\\n Recall that the output should have shape \"\n",
        "            \"(batch_size, vocabulary_size)\".format(\n",
        "                tuple(out.shape), expected_shape\n",
        "            )\n",
        "        )\n",
        "        assert h.shape == (1, batch_size, self.hidden_size), (\n",
        "            \"The shape of final hidden state is invalid. Got shape {0}.\"\n",
        "            \"\\n  Expected shape: {1}\\n Recall that the output should have shape \"\n",
        "            \"(1, batch_size, hidden_size)\".format(\n",
        "                tuple(h.shape), (1, batch_size, self.hidden_size)\n",
        "            )\n",
        "        )\n",
        "        o, s = self.model(tokens, tokens)\n",
        "        assert o.shape == (sequence_length, batch_size, self.vocabulary_size), (\n",
        "            \"The shape of Seq2Seq output is invalid. Got shape {0}.\"\n",
        "            \"\\n  Expected shape: {1}\\n Recall that the output should have shape \"\n",
        "            \"(sequence_length, batch_size, vocabulary_size)\".format(\n",
        "                tuple(o.shape), (sequence_length, batch_size, self.vocabulary_size)\n",
        "            )\n",
        "        )\n",
        "        assert s.shape == (sequence_length, batch_size), (\n",
        "            \"The shape of Seq2Seq sentences is invalid. Got shape {0}.\"\n",
        "            \"\\n  Expected shape: {1}\\n Recall that this should have shape \"\n",
        "            \"(sequence_length, batch_size)\".format(\n",
        "                tuple(s.shape), (sequence_length, batch_size)\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "DSmaQQJqr7xW"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    TestSeq2SeqShapes().test_forward_shape()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AvIsWVsqr7xW"
      },
      "source": [
        "### Transformer Encoder-Decoder Model for Machine Translation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8PSGSqyr7xX"
      },
      "source": [
        "#### Dataloader<br>\n",
        "<br>\n",
        "Running this cell will create `train_loader`, `val_loader` and `test_loader` for training, validation and testing respectively. The batch size is 128 and all sequences are of length 60 – shorter sequences are padded to this length. For this question, the data is provided in the shape `[batch_size, sequence_length]`. You need not modify this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "-E7OYnftr7xY"
      },
      "outputs": [],
      "source": [
        "special_idx = {\n",
        "    \"bos\": en_vocabulary[\"<bos>\"],\n",
        "    \"pad\": en_vocabulary[\"<pad>\"],\n",
        "    \"eos\": en_vocabulary[\"<eos>\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "AuNB61oir7xZ"
      },
      "outputs": [],
      "source": [
        "max_len = 60"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "O_iRAWpgr7xZ"
      },
      "outputs": [],
      "source": [
        "def get_batch(data):\n",
        "    en_batch_, fr_batch_ = [], []\n",
        "    for en_item, fr_item in data:\n",
        "        en_batch_.append(torch.cat([torch.tensor([special_idx[\"bos\"]]), en_item, torch.tensor([special_idx[\"eos\"]])], dim=0))\n",
        "        fr_batch_.append(torch.cat([torch.tensor([special_idx[\"bos\"]]), fr_item, torch.tensor([special_idx[\"eos\"]])], dim=0))\n",
        "    en_batch_ = nn.utils.rnn.pad_sequence(en_batch_, padding_value=special_idx[\"pad\"], batch_first=True)\n",
        "    fr_batch_ = nn.utils.rnn.pad_sequence(fr_batch_, padding_value=special_idx[\"pad\"], batch_first=True)\n",
        "    en_batch = torch.full((en_batch_.shape[0], max_len), special_idx[\"pad\"])\n",
        "    en_batch[:, :en_batch_.shape[1]] = en_batch_\n",
        "    fr_batch = torch.full((fr_batch_.shape[0], max_len), special_idx[\"pad\"])\n",
        "    fr_batch[:, :fr_batch_.shape[1]] = fr_batch_\n",
        "    return en_batch, fr_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "aV28bejSr7xa"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=get_batch)\n",
        "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True, collate_fn=get_batch)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=get_batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG6lzF6Br7xb"
      },
      "source": [
        "#### Embeddings<br>\n",
        "<br>\n",
        "The classes below are used to get the Transformer embeddings. The embeddings are made up of positional encodings and token embeddings. You do not need to modify the code, and you may run it by just clicking the run/play button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "Z6si7isVr7xb"
      },
      "outputs": [],
      "source": [
        "class PostionalEncoding(nn.Module):\n",
        "    \"\"\"\n",
        "    Provides positional information for tokens, which is to be added with the token embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, max_len=max_len, device=device):\n",
        "        super(PostionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, d_model, device=device)\n",
        "        self.encoding.requires_grad = False\n",
        "        pos = torch.arange(0, max_len, device=device)\n",
        "        pos = pos.float().unsqueeze(dim=1)\n",
        "        _2i = torch.arange(0, d_model, step=2, device=device).float()\n",
        "        self.encoding[:, 0::2] = torch.sin(pos / (10000 ** (_2i / d_model)))\n",
        "        self.encoding[:, 1::2] = torch.cos(pos / (10000 ** (_2i / d_model)))\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len = x.size()\n",
        "        return self.encoding[:seq_len, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "40sA8Z2er7xc"
      },
      "outputs": [],
      "source": [
        "class TransformerEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Gives embeddings made up of token embeddings + positional encodings.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabulary_size, d_model, max_len=max_len, use_dropout=False, p_dropout=0.1, device=device):\n",
        "        super(TransformerEmbedding, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocabulary_size, d_model, padding_idx=special_idx[\"pad\"])\n",
        "        self.positional_encoding = PostionalEncoding(d_model, max_len, device)\n",
        "        self.use_dropout = use_dropout\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(p=p_dropout)\n",
        "    def forward(self, x):\n",
        "        token_embeddings = self.token_embedding(x)\n",
        "        positional_encodings = self.positional_encoding(x)\n",
        "        embeddings = token_embeddings + positional_encodings\n",
        "        if self.use_dropout:\n",
        "            embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZuJxrseKr7xd"
      },
      "source": [
        "#### LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "mTCec7cjr7xd"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements layer normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size, eps=1e-5):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.eps = eps\n",
        "        self.weight = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.bias = nn.Parameter(torch.Tensor(hidden_size))\n",
        "        self.reset_parameters()\n",
        "    def forward(self, x):\n",
        "        \"\"\"Layer Normalization.\n",
        "        This module applies Layer Normalization, with rescaling and shift,\n",
        "        only on the last dimension. See Lecture 07, slide 20.\n",
        "        Parameters\n",
        "        ----------\n",
        "        inputs (`torch.FloatTensor` of shape `(*dims, hidden_size)`)\n",
        "            The input tensor. This tensor can have an arbitrary number N of\n",
        "            dimensions, as long as `inputs.shape[N-1] == hidden_size`. The\n",
        "            leading N - 1 dimensions `dims` can be arbitrary.\n",
        "        Returns\n",
        "        -------\n",
        "        outputs (`torch.FloatTensor` of shape `(*dims, hidden_size)`)\n",
        "            The output tensor, having the same shape as `inputs`.\n",
        "        \"\"\"\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = ((x - mean) ** 2).mean(dim=-1, keepdim=True)\n",
        "        std = torch.sqrt(var + self.eps)\n",
        "        y = (x - mean) / std\n",
        "        return self.weight * y + self.bias\n",
        "    def reset_parameters(self):\n",
        "        nn.init.ones_(self.weight)\n",
        "        nn.init.zeros_(self.bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AsdQfb3fr7xe"
      },
      "source": [
        "#### Masked Multi-Head Attention<br>\n",
        "<br>\n",
        "We use masked multi-head attention because the decoder should not attend to future tokens in the target while performing autoregressive generation (machine translation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "NyveSzdGr7xe"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, head_size, num_heads):\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.head_size = head_size\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        # Use nn.Linear objects so as to apply both weights and biases\n",
        "        self.w_q = nn.Linear(head_size * num_heads, head_size * num_heads)\n",
        "        self.w_k = nn.Linear(head_size * num_heads, head_size * num_heads)\n",
        "        self.w_v = nn.Linear(head_size * num_heads, head_size * num_heads)\n",
        "        self.w_y = nn.Linear(head_size * num_heads, head_size * num_heads)\n",
        "        self.b_q = nn.Parameter(torch.zeros(head_size * num_heads))\n",
        "        self.b_k = nn.Parameter(torch.zeros(head_size * num_heads))\n",
        "        self.b_v = nn.Parameter(torch.zeros(head_size * num_heads))\n",
        "        self.b_y = nn.Parameter(torch.zeros(head_size * num_heads))\n",
        "    def get_attention_weights(self, queries, keys, mask=None):\n",
        "        \"\"\"Compute the attention weights.\n",
        "        This computes the attention weights for all the sequences and all the\n",
        "        heads in the batch. For a single sequence and a single head (for\n",
        "        simplicity), if Q are the queries (matrix of size `(sequence_length, head_size)`),\n",
        "        and K are the keys (matrix of size `(sequence_length, head_size)`), then\n",
        "        the attention weights are computed as\n",
        "            x = Q * K^{T} / sqrt(head_size)\n",
        "            weights = softmax(x, mask) = softmax(x . mask - 10^4 (1 - mask))\n",
        "            If mask is None,\n",
        "            weights = softmax(x)\n",
        "        Here \"*\" is the matrix multiplication and \".\" is element-wise multiplication. See Lecture 06, slides 05-13.\n",
        "        Parameters\n",
        "        ----------\n",
        "        queries (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, head_size)`)\n",
        "            Tensor containing the queries for all the positions in the sequences\n",
        "            and all the heads.\n",
        "        keys (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, head_size)`)\n",
        "            Tensor containing the keys for all the positions in the sequences\n",
        "            and all the heads.\n",
        "        mask (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, sequence_length)`)\n",
        "            Tensor containing the mask. Default: None.\n",
        "        Returns\n",
        "        -------\n",
        "        attention_weights (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, sequence_length)`)\n",
        "            Tensor containing the attention weights for all the heads and all\n",
        "            the sequences in the batch.\n",
        "        \"\"\"\n",
        "        if mask is not None:\n",
        "            mask = mask.int()\n",
        "        batch_size, num_heads, sequence_length, head_size = queries.size()\n",
        "        attention_weights = torch.matmul(queries, keys.transpose(-1, -2)) / np.sqrt(head_size)\n",
        "        if mask is not None:\n",
        "            attention_weights = attention_weights * mask - 10**4 * (1 - mask)\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "        return attention_weights\n",
        "            \n",
        "    def apply_attention(self, queries, keys, values, mask=None):\n",
        "        \"\"\"Apply the attention.\n",
        "        This computes the output of the attention, for all the sequences and\n",
        "        all the heads in the batch. For a single sequence and a single head\n",
        "        (for simplicity), if Q are the queries (matrix of size `(sequence_length, head_size)`),\n",
        "        K are the keys (matrix of size `(sequence_length, head_size)`), and V are\n",
        "        the values (matrix of size `(sequence_length, head_size)`), then the ouput\n",
        "        of the attention is given by\n",
        "            weights = softmax(Q * K^{T} / sqrt(head_size), mask)\n",
        "            attended_values = weights * V\n",
        "            outputs = concat(attended_values)\n",
        "        Here \"*\" is the matrix multiplication, and \"concat\" is the operation\n",
        "        that concatenates the attended values of all the heads (see the\n",
        "        `merge_heads` function). See Lecture 06, slides 05-13.\n",
        "        Parameters\n",
        "        ----------\n",
        "        queries (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, head_size)`)\n",
        "            Tensor containing the queries for all the positions in the sequences\n",
        "            and all the heads. \n",
        "        keys (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, head_size)`)\n",
        "            Tensor containing the keys for all the positions in the sequences\n",
        "            and all the heads. \n",
        "        values (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, head_size)`)\n",
        "            Tensor containing the values for all the positions in the sequences\n",
        "            and all the heads. \n",
        "        mask (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, sequence_length)`)\n",
        "            Tensor containing the mask. Default: None.\n",
        "        Returns\n",
        "        -------\n",
        "        outputs (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_heads * head_size)`)\n",
        "            Tensor containing the concatenated outputs of the attention for all\n",
        "            the sequences in the batch, and all positions in each sequence. \n",
        "        \"\"\"\n",
        "        attention_weights = self.get_attention_weights(queries, keys, mask)\n",
        "        outputs = torch.matmul(attention_weights, values)\n",
        "        return self.merge_heads(outputs)\n",
        "    def split_heads(self, tensor):\n",
        "        \"\"\"Split the head vectors.\n",
        "        This function splits the head vectors that have been concatenated (e.g.\n",
        "        through the `merge_heads` function) into a separate dimension. This\n",
        "        function also transposes the `sequence_length` and `num_heads` axes.\n",
        "        It only reshapes and transposes the input tensor, and it does not\n",
        "        apply any further transformation to the tensor. The function `split_heads`\n",
        "        is the inverse of the function `merge_heads`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        tensor (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_heads * dim)`)\n",
        "            Input tensor containing the concatenated head vectors (each having\n",
        "            a size `dim`, which can be arbitrary).\n",
        "        Returns\n",
        "        -------\n",
        "        output (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, dim)`)\n",
        "            Reshaped and transposed tensor containing the separated head\n",
        "            vectors. Here `dim` is the same dimension as the one in the\n",
        "            definition of the input `tensor` above.\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, num_heads_dim = tensor.size()\n",
        "        dim, num_heads = num_heads_dim // self.num_heads, self.num_heads\n",
        "        tensor = tensor.view(batch_size, sequence_length, num_heads, dim)\n",
        "        tensor = tensor.transpose(1, 2)\n",
        "        return tensor\n",
        "    def merge_heads(self, tensor):\n",
        "        \"\"\"Merge the head vectors.\n",
        "        This function concatenates the head vectors in a single vector. This\n",
        "        function also transposes the `sequence_length` and the newly created\n",
        "        \"merged\" dimension. It only reshapes and transposes the input tensor,\n",
        "        and it does not apply any further transformation to the tensor. The\n",
        "        function `merge_heads` is the inverse of the function `split_heads`.\n",
        "        Parameters\n",
        "        ----------\n",
        "        tensor (`torch.FloatTensor` of shape `(batch_size, num_heads, sequence_length, dim)`)\n",
        "            Input tensor containing the separated head vectors (each having\n",
        "            a size `dim`, which can be arbitrary).\n",
        "        Returns\n",
        "        -------\n",
        "        output (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_heads * dim)`)\n",
        "            Reshaped and transposed tensor containing the concatenated head\n",
        "            vectors. Here `dim` is the same dimension as the one in the\n",
        "            definition of the input `tensor` above.\n",
        "        \"\"\"\n",
        "        batch_size, num_heads, sequence_length, dim = tensor.size()\n",
        "        tensor = tensor.transpose(1, 2)\n",
        "        tensor = tensor.reshape(batch_size, sequence_length, num_heads * dim)\n",
        "        return tensor\n",
        "    def forward(self, queries, keys, values, mask=None):\n",
        "        \"\"\"Multi-headed attention.\n",
        "        This applies the multi-headed attention on the input tensors `hidden_states`.\n",
        "        For a single sequence (for simplicity), if X are the hidden states from\n",
        "        the previous layer (a matrix of size `(sequence_length, num_heads * head_size)`\n",
        "        containing the concatenated head vectors), then the output of multi-headed\n",
        "        attention is given by\n",
        "            Q = X * W_{Q} + b_{Q}        # Queries\n",
        "            K = X * W_{K} + b_{K}        # Keys\n",
        "            V = X * W_{V} + b_{V}        # Values\n",
        "            Y = attention(Q, K, V, mask) # Attended values (concatenated for all heads)\n",
        "            outputs = Y * W_{Y} + b_{Y}  # Linear projection\n",
        "        Here \"*\" is the matrix multiplication.\n",
        "        Parameters\n",
        "        ----------\n",
        "        hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_heads * head_size)`)\n",
        "            Input tensor containing the concatenated head vectors for all the\n",
        "            sequences in the batch, and all positions in each sequence. This\n",
        "            is, for example, the tensor returned by the previous layer.\n",
        "        Returns\n",
        "        -------\n",
        "        output (`torch.FloatTensor` of shape `(batch_size, sequence_length, num_heads * head_size)`)\n",
        "            Tensor containing the output of multi-headed attention for all the\n",
        "            sequences in the batch, and all positions in each sequence.\n",
        "        \"\"\"\n",
        "        queries = self.w_q(queries) + self.b_q\n",
        "        keys = self.w_k(keys) + self.b_k\n",
        "        values = self.w_v(values) + self.b_v\n",
        "        queries = self.split_heads(queries)\n",
        "        keys = self.split_heads(keys)\n",
        "        values = self.split_heads(values)\n",
        "        outputs = self.apply_attention(queries, keys, values, mask)\n",
        "        outputs = self.w_y(outputs) + self.b_y\n",
        "        return outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xrJYvXApr7xf"
      },
      "source": [
        "#### Position-wise FFN<br>\n",
        "Defines a feedforward network, do not modify. You may just run it by clicking the run/play button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "NofIFONgr7xg"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements a position-wise FFN.\n",
        "    \"\"\"\n",
        "    def __init__(self, d_model, hidden_size, use_dropout=False, p_dropout=0.1):\n",
        "        super(PositionwiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.use_dropout = use_dropout\n",
        "        if self.use_dropout:\n",
        "            self.dropout = nn.Dropout(p=p_dropout)\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.relu(x)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qVQPromr7xg"
      },
      "source": [
        "#### EncoderLayer and DecoderLayer<br>\n",
        "<br>\n",
        "Defines a layer of the Encoder or Decoder respectively, do not modify. You may just run it by clicking the run/play button.<br>\n",
        "<br>\n",
        "The multi-head attention in the encoder attends over all tokens in the source strings (input for translation). The decoder is comprised of 2 multi-head attention steps: one to attend over the (allowed) tokens from the target sentence, and one to attend over the encoder outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "C6kC_8Omr7xh"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a layer of the encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size, hidden_size, num_heads, use_dropout=False, p_dropout=0.1):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.attention = MultiHeadAttention(head_size, num_heads)\n",
        "        self.norm1 = LayerNorm(head_size * num_heads)\n",
        "        self.use_dropout = use_dropout\n",
        "        if self.use_dropout:\n",
        "            self.dropout1 = nn.Dropout(p=p_dropout)\n",
        "        self.ffn = PositionwiseFeedForward(head_size * num_heads, hidden_size, use_dropout, p_dropout)\n",
        "        self.norm2 = LayerNorm(head_size * num_heads)\n",
        "        if self.use_dropout:\n",
        "            self.dropout2 = nn.Dropout(p=p_dropout)\n",
        "    def forward(self, x, mask):\n",
        "        x_ = x\n",
        "        x = self.attention(x, x, x, mask=mask)\n",
        "        x = self.norm1(x + x_)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout1(x)\n",
        "        x_ = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.norm2(x + x_)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "HXeRrxzMr7xh"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines a layer of the decoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, head_size, hidden_size, num_heads, use_dropout=False, p_dropout=0.1):\n",
        "        super(DecoderLayer, self).__init__()\n",
        "        self.self_attention = MultiHeadAttention(head_size, num_heads)\n",
        "        self.norm1 = LayerNorm(head_size * num_heads)\n",
        "        self.use_dropout = use_dropout\n",
        "        if self.use_dropout:\n",
        "            self.dropout1 = nn.Dropout(p=p_dropout)\n",
        "        self.encoder_decoder_attention = MultiHeadAttention(head_size, num_heads)\n",
        "        self.norm2 = LayerNorm(head_size * num_heads)\n",
        "        if self.use_dropout:\n",
        "            self.dropout2 = nn.Dropout(p=p_dropout)\n",
        "        self.ffn = PositionwiseFeedForward(head_size * num_heads, hidden_size, use_dropout, p_dropout)\n",
        "        self.norm3 = LayerNorm(head_size * num_heads)\n",
        "        if self.use_dropout:\n",
        "            self.dropout3 = nn.Dropout(p=p_dropout)\n",
        "    def forward(self, x_decoder, x_encoder, mask_target, mask_source):\n",
        "        x_ = x_decoder\n",
        "        x = self.self_attention(x_decoder, x_decoder, x_decoder, mask=mask_target)\n",
        "        x = self.norm1(x + x_)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout1(x)\n",
        "        if x_encoder is not None:\n",
        "            x_ = x\n",
        "            x = self.encoder_decoder_attention(x, x_encoder, x_encoder, mask=mask_source)\n",
        "            x = self.norm2(x + x_)\n",
        "            if self.use_dropout:\n",
        "                x = self.dropout2(x)\n",
        "        x_ = x\n",
        "        x = self.ffn(x)\n",
        "        x = self.norm3(x + x_)\n",
        "        if self.use_dropout:\n",
        "            x = self.dropout3(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaktnWgWr7xi"
      },
      "source": [
        "#### TEncoder and TDecoder<br>\n",
        "<br>\n",
        "Defines the Encoder and Decoder of the Transformer. You will have to fill in the `forward` methods of both classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "5KyfTYF8r7xi"
      },
      "outputs": [],
      "source": [
        "class TEncoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines the encoder of the Transformer model.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocabulary_size=10838,\n",
        "                 max_len=max_len,\n",
        "                 head_size=64,\n",
        "                 hidden_size=512,\n",
        "                 num_heads=8,\n",
        "                 num_layers=6,\n",
        "                 use_dropout=False,\n",
        "                 p_dropout=0.1,\n",
        "                 device=device):\n",
        "        super(TEncoder, self).__init__()\n",
        "        self.embedding = TransformerEmbedding(d_model=head_size * num_heads,\n",
        "                                              max_len=max_len,\n",
        "                                              vocabulary_size=vocabulary_size,\n",
        "                                              use_dropout=use_dropout,\n",
        "                                              p_dropout=p_dropout,\n",
        "                                              device=device)\n",
        "        self.layers = nn.ModuleList([EncoderLayer(head_size=head_size,\n",
        "                                                  hidden_size=hidden_size,\n",
        "                                                  num_heads=num_heads,\n",
        "                                                  use_dropout=use_dropout,\n",
        "                                                  p_dropout=p_dropout)\n",
        "                                     for _ in range(num_layers)])\n",
        "    def forward(self, x, mask):\n",
        "        # First get embeddings for x.\n",
        "        # Then, get the output of each layer.\n",
        "        # x -> embed -> layer[0] -> layer[1] -> ... -> layer[n-1] -> return output.\n",
        "        # WRITE CODE HERE\n",
        "        x = self.embedding(x)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "MAwD2Oapr7xj"
      },
      "outputs": [],
      "source": [
        "class TDecoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Defines the decoder of the Transformer model.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 vocabulary_size=11510,\n",
        "                 max_len=max_len,\n",
        "                 head_size=64,\n",
        "                 hidden_size=512,\n",
        "                 num_heads=8,\n",
        "                 num_layers=6,\n",
        "                 use_dropout=False,\n",
        "                 p_dropout=0.1,\n",
        "                 device=device):\n",
        "        super(TDecoder, self).__init__()\n",
        "        self.embedding = TransformerEmbedding(d_model=head_size * num_heads,\n",
        "                                              max_len=max_len,\n",
        "                                              vocabulary_size=vocabulary_size,\n",
        "                                              use_dropout=use_dropout,\n",
        "                                              p_dropout=p_dropout,\n",
        "                                              device=device)\n",
        "        self.layers = nn.ModuleList([DecoderLayer(head_size=head_size,\n",
        "                                                  hidden_size=hidden_size,\n",
        "                                                  num_heads=num_heads,\n",
        "                                                  use_dropout=use_dropout,\n",
        "                                                  p_dropout=p_dropout)\n",
        "                                     for _ in range(num_layers)])\n",
        "        self.linear = nn.Linear(head_size * num_heads, vocabulary_size)\n",
        "    def forward(self, target, encoded_source, mask_target, mask_source):\n",
        "        # First, get embeddings for target.\n",
        "        # Then, get the output of each layer.\n",
        "        # target -> embed -> layer[0] -> layer[1] -> ... -> layer[n-1] -> final layer output.\n",
        "        # Pass final layer output to linear -> return final output.\n",
        "        # WRITE CODE HERE\n",
        "        target = self.embedding(target)\n",
        "        for layer in self.layers:\n",
        "            target = layer(target, encoded_source, mask_target, mask_source)\n",
        "        target = self.linear(target)\n",
        "        return target"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QV1LRAwr7xj"
      },
      "source": [
        "#### Transformer<br>\n",
        "<br>\n",
        "This class defines the encoder-decoder Transformer model, do not modify. You may just run the cell by clicking the run/play button."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "dKA-ClzRr7xk"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements an encoder-decoder transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 enc_vocabulary_size=10838,\n",
        "                 dec_vocabulary_size=11510,\n",
        "                 head_size=64,\n",
        "                 num_heads=8,\n",
        "                 max_len=max_len,\n",
        "                 hidden_size=512,\n",
        "                 num_layers=6,\n",
        "                 use_dropout=False,\n",
        "                 p_dropout=0.1,\n",
        "                 strategy=\"greedy\",\n",
        "                 device=device):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.pad_idx = special_idx[\"pad\"]\n",
        "        self.sos_idx = special_idx[\"bos\"]\n",
        "        self.strategy = strategy\n",
        "        self.device = device\n",
        "        self.encoder = TEncoder(head_size=head_size,\n",
        "                                num_heads=num_heads,\n",
        "                                max_len=max_len,\n",
        "                                hidden_size=hidden_size,\n",
        "                                vocabulary_size=enc_vocabulary_size,\n",
        "                                use_dropout=use_dropout,\n",
        "                                p_dropout=p_dropout,\n",
        "                                num_layers=num_layers,\n",
        "                                device=device)\n",
        "        self.decoder = TDecoder(head_size=head_size,\n",
        "                                num_heads=num_heads,\n",
        "                                max_len=max_len,\n",
        "                                hidden_size=hidden_size,\n",
        "                                vocabulary_size=dec_vocabulary_size,\n",
        "                                use_dropout=use_dropout,\n",
        "                                p_dropout=p_dropout,\n",
        "                                num_layers=num_layers,\n",
        "                                device=device)\n",
        "    def forward(self, source, target):\n",
        "        mask_source = self.make_pad_mask(source, source)\n",
        "        mask_source_target = self.make_pad_mask(target, source)\n",
        "        mask_target = self.make_pad_mask(target, target) * self.make_no_peak_mask(target, target)\n",
        "        encoded_source = self.encoder(source, mask_source)\n",
        "        output = self.decoder(target, encoded_source, mask_target, mask_source_target)\n",
        "        return output\n",
        "    def make_pad_mask(self, q, k):\n",
        "        len_q, len_k = q.size(1), k.size(1)\n",
        "        k = k.ne(self.pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "        k = k.repeat(1, 1, len_q, 1)\n",
        "        q = q.ne(self.pad_idx).unsqueeze(1).unsqueeze(3)\n",
        "        q = q.repeat(1, 1, 1, len_k)\n",
        "        mask = k & q\n",
        "        return mask\n",
        "    def make_no_peak_mask(self, q, k):\n",
        "        len_q, len_k = q.size(1), k.size(1)\n",
        "        mask = torch.tril(torch.ones(len_q, len_k)).type(torch.BoolTensor).to(self.device)\n",
        "        return mask"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QymgdNIr7xk"
      },
      "source": [
        "#### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "ARWxdyaqr7xl"
      },
      "outputs": [],
      "source": [
        "def train_transformer(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    train_loss = 0.\n",
        "    train_ppl = 0.\n",
        "    start = time.time()\n",
        "    for i, (source, target) in enumerate(dataloader):\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(source, target[:, :-1])\n",
        "        output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "        target = target[:, 1:].contiguous().view(-1)\n",
        "        loss = criterion(output_reshape, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "    end = time.time()\n",
        "    train_loss /= len(dataloader)\n",
        "    train_ppl = math.exp(train_loss)\n",
        "    train_time = end - start\n",
        "    return train_loss, train_ppl, train_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "nysHsrjmr7xl"
      },
      "outputs": [],
      "source": [
        "def validate_transformer(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    val_loss = 0.\n",
        "    val_ppl = 0.\n",
        "    start = time.time()\n",
        "    with torch.no_grad():\n",
        "        for i, (source, target) in enumerate(dataloader):\n",
        "            source, target = source.to(device), target.to(device)\n",
        "            output = model(source, target[:, :-1])\n",
        "            output_reshape = output.contiguous().view(-1, output.shape[-1])\n",
        "            target = target[:, 1:].contiguous().view(-1)\n",
        "            loss = criterion(output_reshape, target)\n",
        "            val_loss += loss.item()\n",
        "    end = time.time()\n",
        "    val_loss /= len(dataloader)\n",
        "    val_ppl = math.exp(val_loss)\n",
        "    val_time = end - start\n",
        "    return val_loss, val_ppl, val_time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3Qxb54Vwr7xm"
      },
      "outputs": [],
      "source": [
        "def compute_bleu(model, dataloader):\n",
        "    targets = []\n",
        "    translations = []\n",
        "    for source, target in dataloader:\n",
        "        source, target = source.to(device), target.to(device)\n",
        "        output = model(source, target)\n",
        "        for i in range(target.shape[0]):\n",
        "            targets.append([[]])\n",
        "            for token in target[i, 1:]:\n",
        "                if token in special_idx.values():\n",
        "                    break\n",
        "                targets[-1][0].append(fr_vocabulary.get_itos()[token])\n",
        "            translations.append([])\n",
        "            if model.strategy == \"greedy\":\n",
        "                # WRITE CODE HERE\n",
        "                # Assign the greedy choice tokens for output[i] to o\n",
        "                # output[i] has shape (sequence_length, vocabulary_size)\n",
        "                # o should have shape (sequence_length)\n",
        "                o = output[i].argmax(dim=1)\n",
        "            elif model.strategy == \"random\":\n",
        "                # WRITE CODE HERE\n",
        "                # Assign the randomly sampled tokens for output[i] to o\n",
        "                # output[i] has shape (sequence_length, vocabulary_size)\n",
        "                # o should have shape (sequence_length)\n",
        "                temperature = 0.5\n",
        "                o = torch.multinomial(torch.softmax(output[i] / temperature, dim=1), 1).squeeze(1)\n",
        "            for token in o:\n",
        "                if token in special_idx.values():\n",
        "                    break\n",
        "                translations[-1].append(fr_vocabulary.get_itos()[token.int().item()])\n",
        "        break\n",
        "    return torchtext.data.metrics.bleu_score(translations, targets, max_n=2, weights=[0.5]*2) * 100,             torchtext.data.metrics.bleu_score(translations, targets, max_n=1, weights=[1]) * 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "16hzb_BQr7xm"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = False\n",
        "strategy = \"greedy\"\n",
        "model = Transformer(head_size=64,\n",
        "                    enc_vocabulary_size=10838,\n",
        "                    dec_vocabulary_size=11510,\n",
        "                    max_len=max_len,\n",
        "                    hidden_size=512,\n",
        "                    num_heads=8,\n",
        "                    num_layers=6,\n",
        "                    use_dropout=use_dropout,\n",
        "                    p_dropout=0.1,\n",
        "                    strategy=strategy,\n",
        "                    device=device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "zjPkeK0kr7xn"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\" : [],\n",
        "    \"info_free\" : [],\n",
        "    \"info_used\" : [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PA_8OOCUr7xn",
        "outputId": "7e67b7ab-52ce-4027-ec64-f64e2d738b1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 6.696 | Train PPL: 809.011 | Train Time: 146.544\n",
            "Valid Loss: 5.596 | Valid PPL: 269.358 | Valid Time: 1.904\n",
            "Memory : (77.54% free): 15843721216(total), 12284854272 (free), 3558866944 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 5.030 | Train PPL: 152.873 | Train Time: 145.788\n",
            "Valid Loss: 4.588 | Valid PPL:  98.306 | Valid Time: 1.906\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 4.294 | Train PPL:  73.260 | Train Time: 146.158\n",
            "Valid Loss: 4.058 | Valid PPL:  57.876 | Valid Time: 1.914\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 3.876 | Train PPL:  48.216 | Train Time: 146.325\n",
            "Valid Loss: 3.724 | Valid PPL:  41.437 | Valid Time: 1.903\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 3.580 | Train PPL:  35.883 | Train Time: 146.337\n",
            "Valid Loss: 3.477 | Valid PPL:  32.364 | Valid Time: 1.907\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 3.355 | Train PPL:  28.633 | Train Time: 146.164\n",
            "Valid Loss: 3.285 | Valid PPL:  26.707 | Valid Time: 1.910\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 3.169 | Train PPL:  23.787 | Train Time: 146.580\n",
            "Valid Loss: 3.121 | Valid PPL:  22.668 | Valid Time: 1.904\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 3.008 | Train PPL:  20.247 | Train Time: 146.364\n",
            "Valid Loss: 2.981 | Valid PPL:  19.702 | Valid Time: 1.894\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 2.869 | Train PPL:  17.617 | Train Time: 146.309\n",
            "Valid Loss: 2.858 | Valid PPL:  17.432 | Valid Time: 1.910\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 2.744 | Train PPL:  15.551 | Train Time: 146.508\n",
            "Valid Loss: 2.754 | Valid PPL:  15.707 | Valid Time: 1.906\n",
            "Memory : (54.24% free): 15843721216(total), 8593866752 (free), 7249854464 (used)\n",
            "== Test Loss: 2.693 | Test PPL:  14.778 | Test Time: 1.864 ==\n",
            "Test BLEU-1: 45.301395654678345 Test BLEU-2: 59.45674180984497\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train_transformer(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate_transformer(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate_transformer(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"transformer-adam-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "k8vd0b_Dr7xo"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = True\n",
        "strategy = \"greedy\"\n",
        "model = Transformer(head_size=64,\n",
        "                    enc_vocabulary_size=10838,\n",
        "                    dec_vocabulary_size=11510,\n",
        "                    max_len=max_len,\n",
        "                    hidden_size=512,\n",
        "                    num_heads=8,\n",
        "                    num_layers=6,\n",
        "                    use_dropout=use_dropout,\n",
        "                    p_dropout=0.1,\n",
        "                    strategy=strategy,\n",
        "                    device=device).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "IXMshotjr7xo"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\" : [],\n",
        "    \"info_free\" : [],\n",
        "    \"info_used\" : [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2PLJqtlr7xp",
        "outputId": "f3de47d7-c892-4a1b-fbd2-e034926eef78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 9.574 | Train PPL: 14389.618 | Train Time: 143.898\n",
            "Valid Loss: 9.574 | Valid PPL: 14380.819 | Valid Time: 1.917\n",
            "Memory : (53.74% free): 15843721216(total), 8514174976 (free), 7329546240 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 9.563 | Train PPL: 14234.051 | Train Time: 143.670\n",
            "Valid Loss: 9.551 | Valid PPL: 14065.371 | Valid Time: 1.917\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 9.550 | Train PPL: 14044.555 | Train Time: 143.514\n",
            "Valid Loss: 9.530 | Valid PPL: 13761.913 | Valid Time: 1.900\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 9.541 | Train PPL: 13925.606 | Train Time: 143.823\n",
            "Valid Loss: 9.508 | Valid PPL: 13463.502 | Valid Time: 1.901\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 9.530 | Train PPL: 13763.911 | Train Time: 144.045\n",
            "Valid Loss: 9.486 | Valid PPL: 13167.496 | Valid Time: 1.921\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 9.519 | Train PPL: 13614.500 | Train Time: 144.193\n",
            "Valid Loss: 9.464 | Valid PPL: 12886.134 | Valid Time: 1.926\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 9.508 | Train PPL: 13473.728 | Train Time: 143.528\n",
            "Valid Loss: 9.442 | Valid PPL: 12605.953 | Valid Time: 1.901\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 9.498 | Train PPL: 13333.911 | Train Time: 143.553\n",
            "Valid Loss: 9.420 | Valid PPL: 12333.074 | Valid Time: 1.911\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 9.486 | Train PPL: 13170.520 | Train Time: 143.607\n",
            "Valid Loss: 9.398 | Valid PPL: 12065.208 | Valid Time: 1.926\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 9.475 | Train PPL: 13035.187 | Train Time: 143.715\n",
            "Valid Loss: 9.376 | Valid PPL: 11803.990 | Valid Time: 1.899\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "== Test Loss: 9.367 | Test PPL: 11695.509 | Test Time: 1.873 ==\n",
            "Test BLEU-1: 0.0 Test BLEU-2: 0.02604167675599456\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train_transformer(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate_transformer(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate_transformer(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"transformer-sgd-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "nLAy_-izr7xp"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = True\n",
        "strategy = \"greedy\"\n",
        "model = Transformer(head_size=64,\n",
        "                    enc_vocabulary_size=10838,\n",
        "                    dec_vocabulary_size=11510,\n",
        "                    max_len=max_len,\n",
        "                    hidden_size=512,\n",
        "                    num_heads=8,\n",
        "                    num_layers=6,\n",
        "                    use_dropout=use_dropout,\n",
        "                    p_dropout=0.1,\n",
        "                    strategy=strategy,\n",
        "                    device=device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "V69cZHxgr7xp"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\" : [],\n",
        "    \"info_free\" : [],\n",
        "    \"info_used\" : [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AgcklQDQr7xq",
        "outputId": "f91c903a-3999-4c83-f1ca-20e0af6c0d9c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 7.524 | Train PPL: 1852.260 | Train Time: 149.354\n",
            "Valid Loss: 6.254 | Valid PPL: 519.865 | Valid Time: 1.897\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 5.825 | Train PPL: 338.748 | Train Time: 148.976\n",
            "Valid Loss: 5.193 | Valid PPL: 180.060 | Valid Time: 1.912\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 5.100 | Train PPL: 163.965 | Train Time: 148.993\n",
            "Valid Loss: 4.856 | Valid PPL: 128.496 | Valid Time: 1.910\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 4.796 | Train PPL: 121.069 | Train Time: 148.943\n",
            "Valid Loss: 4.565 | Valid PPL:  96.062 | Valid Time: 1.909\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 4.529 | Train PPL:  92.678 | Train Time: 149.203\n",
            "Valid Loss: 4.325 | Valid PPL:  75.580 | Valid Time: 1.921\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 4.328 | Train PPL:  75.802 | Train Time: 149.320\n",
            "Valid Loss: 4.157 | Valid PPL:  63.899 | Valid Time: 1.907\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 4.175 | Train PPL:  65.040 | Train Time: 149.178\n",
            "Valid Loss: 4.011 | Valid PPL:  55.207 | Valid Time: 1.914\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 4.049 | Train PPL:  57.347 | Train Time: 149.190\n",
            "Valid Loss: 3.903 | Valid PPL:  49.541 | Valid Time: 1.918\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 3.939 | Train PPL:  51.371 | Train Time: 149.196\n",
            "Valid Loss: 3.803 | Valid PPL:  44.840 | Valid Time: 1.909\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 3.841 | Train PPL:  46.575 | Train Time: 149.249\n",
            "Valid Loss: 3.720 | Valid PPL:  41.259 | Valid Time: 1.913\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "== Test Loss: 3.704 | Test PPL:  40.603 | Test Time: 1.871 ==\n",
            "Test BLEU-1: 23.063665528664497 Test BLEU-2: 36.617229041385876\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train_transformer(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate_transformer(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate_transformer(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"transformer-adam-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "OnmC3ZoSr7xq"
      },
      "outputs": [],
      "source": [
        "fix_experiment_seed()\n",
        "use_dropout = True\n",
        "strategy = \"random\"\n",
        "model = Transformer(head_size=64,\n",
        "                    enc_vocabulary_size=10838,\n",
        "                    dec_vocabulary_size=11510,\n",
        "                    max_len=max_len,\n",
        "                    hidden_size=512,\n",
        "                    num_heads=8,\n",
        "                    num_layers=6,\n",
        "                    use_dropout=use_dropout,\n",
        "                    p_dropout=0.1,\n",
        "                    strategy=strategy,\n",
        "                    device=device).to(device)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
        "criterion = get_criterion()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "pf8IJGFor7xr"
      },
      "outputs": [],
      "source": [
        "results = {\n",
        "    \"train_losses\": [],\n",
        "    \"train_ppls\":   [],\n",
        "    \"train_times\":  [],\n",
        "    \"valid_losses\": [],\n",
        "    \"valid_ppls\":   [],\n",
        "    \"valid_times\":  [],\n",
        "    \"info_total\" : [],\n",
        "    \"info_free\" : [],\n",
        "    \"info_used\" : [],\n",
        "    \"test_loss\":    0.,\n",
        "    \"test_ppl\":     0.,\n",
        "    \"test_bleu1\":   0.,\n",
        "    \"test_bleu2\":   0.,\n",
        "    \"test_time\":    0.\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SaQAXO_r7xt",
        "outputId": "551c84b2-a8ea-4a66-d72f-e546df570959"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0\n",
            "Train Loss: 7.524 | Train PPL: 1852.260 | Train Time: 148.774\n",
            "Valid Loss: 6.254 | Valid PPL: 519.865 | Valid Time: 1.888\n",
            "Memory : (51.54% free): 15843721216(total), 8166047744 (free), 7677673472 (used)\n",
            "Epoch: 1\n",
            "Train Loss: 5.825 | Train PPL: 338.748 | Train Time: 148.994\n",
            "Valid Loss: 5.193 | Valid PPL: 180.060 | Valid Time: 1.905\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 2\n",
            "Train Loss: 5.100 | Train PPL: 163.965 | Train Time: 148.774\n",
            "Valid Loss: 4.856 | Valid PPL: 128.496 | Valid Time: 1.892\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 3\n",
            "Train Loss: 4.796 | Train PPL: 121.069 | Train Time: 148.807\n",
            "Valid Loss: 4.565 | Valid PPL:  96.062 | Valid Time: 1.916\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 4\n",
            "Train Loss: 4.529 | Train PPL:  92.678 | Train Time: 149.029\n",
            "Valid Loss: 4.325 | Valid PPL:  75.580 | Valid Time: 1.917\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 5\n",
            "Train Loss: 4.328 | Train PPL:  75.802 | Train Time: 148.818\n",
            "Valid Loss: 4.157 | Valid PPL:  63.899 | Valid Time: 1.903\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 6\n",
            "Train Loss: 4.175 | Train PPL:  65.040 | Train Time: 148.834\n",
            "Valid Loss: 4.011 | Valid PPL:  55.207 | Valid Time: 1.903\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 7\n",
            "Train Loss: 4.049 | Train PPL:  57.347 | Train Time: 148.894\n",
            "Valid Loss: 3.903 | Valid PPL:  49.541 | Valid Time: 1.904\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 8\n",
            "Train Loss: 3.939 | Train PPL:  51.371 | Train Time: 148.783\n",
            "Valid Loss: 3.803 | Valid PPL:  44.840 | Valid Time: 1.908\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "Epoch: 9\n",
            "Train Loss: 3.841 | Train PPL:  46.575 | Train Time: 148.972\n",
            "Valid Loss: 3.720 | Valid PPL:  41.259 | Valid Time: 1.909\n",
            "Memory : (51.32% free): 15843721216(total), 8130396160 (free), 7713325056 (used)\n",
            "== Test Loss: 3.704 | Test PPL:  40.603 | Test Time: 1.873 ==\n",
            "Test BLEU-1: 22.055968262611312 Test BLEU-2: 37.822267029453826\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    nvidia_smi.nvmlInit()\n",
        "    handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "    n_epochs = 10\n",
        "    for epoch in range(n_epochs):\n",
        "        info = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "        train_loss, train_ppl, train_time = train_transformer(model, train_loader, optimizer, criterion)\n",
        "        valid_loss, valid_ppl, valid_time = validate_transformer(model, val_loader, criterion)\n",
        "        print(f\"Epoch: {epoch}\")\n",
        "        print(f\"Train Loss: {train_loss:.3f} | Train PPL: {train_ppl:7.3f} | Train Time: {train_time:.3f}\")\n",
        "        print(f\"Valid Loss: {valid_loss:.3f} | Valid PPL: {valid_ppl:7.3f} | Valid Time: {valid_time:.3f}\")\n",
        "        results[\"train_losses\"].append(train_loss)\n",
        "        results[\"train_ppls\"].append(train_ppl)\n",
        "        results[\"train_times\"].append(train_time)\n",
        "        results[\"valid_losses\"].append(valid_loss)\n",
        "        results[\"valid_ppls\"].append(valid_ppl)\n",
        "        results[\"valid_times\"].append(valid_time)\n",
        "        results[\"info_total\"].append(info.total)\n",
        "        results[\"info_free\"].append(info.free)\n",
        "        results[\"info_used\"].append(info.used)\n",
        "        print(\"Memory : ({:.2f}% free): {}(total), {} (free), {} (used)\".format(100*info.free/info.total, info.total, info.free, info.used))\n",
        "    test_loss, test_ppl, test_time = validate_transformer(model, test_loader, criterion)\n",
        "    print(f\"== Test Loss: {test_loss:.3f} | Test PPL: {test_ppl:7.3f} | Test Time: {test_time:.3f} ==\")\n",
        "    results[\"test_loss\"] = test_loss\n",
        "    results[\"test_ppl\"] = test_ppl\n",
        "    results[\"test_time\"] = test_time\n",
        "    results[\"test_bleu1\"], results[\"test_bleu2\"] = compute_bleu(model, test_loader)\n",
        "    print(\"Test BLEU-1:\", results[\"test_bleu1\"], \"Test BLEU-2:\", results[\"test_bleu2\"])\n",
        "    json.dump(results, open(f\"transformer-adam-dropout_{str(use_dropout)}-strategy_{strategy}.json\", \"w\"), indent=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnnfpIu3r7xt"
      },
      "source": [
        "#### Basic Tests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "hHkz2C9er7xu"
      },
      "outputs": [],
      "source": [
        "class TestLayerNormShapes():\n",
        "    def __init__(self):\n",
        "        self.hidden_size = 11\n",
        "        self.layer_norm = LayerNorm(self.hidden_size)\n",
        "    def test_forward_shape(self):\n",
        "        shape = (3, 5, 7)\n",
        "        inputs = torch.randn(*shape, self.hidden_size)\n",
        "        outputs = self.layer_norm(inputs)\n",
        "        expected_shape = (*shape, self.hidden_size)\n",
        "        assert isinstance(outputs, torch.Tensor), (\n",
        "            \"The output of the module \" \"must be a torch.Tensor\"\n",
        "        )\n",
        "        assert outputs.shape == expected_shape, (\n",
        "            \"The shape of the output of \"\n",
        "            \"the module is invalid with `inputs` of shape {0}.\\n  Got \"\n",
        "            \"shape {1}\\n  Expected shape: {2}\\nRecall that the output should \"\n",
        "            \"have shape (*dims, hidden_size)\".format(\n",
        "                tuple(inputs.shape), tuple(outputs.shape), expected_shape\n",
        "            )\n",
        "        )\n",
        "if __name__ == \"__main__\":\n",
        "    TestLayerNormShapes().test_forward_shape()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "FupuIUVer7xu"
      },
      "outputs": [],
      "source": [
        "class TestMultiHeadAttentionShapes():\n",
        "    def __init__(self):\n",
        "        self.head_size = 11\n",
        "        self.num_heads = 13\n",
        "        self.sequence_length = 17\n",
        "        self.attention = MultiHeadAttention(\n",
        "            self.head_size, self.num_heads\n",
        "        )\n",
        "    def test_get_attention_weights_shape(self):\n",
        "        batch_size = 7\n",
        "        queries = torch.randn(\n",
        "            batch_size, self.num_heads, self.sequence_length, self.head_size\n",
        "        )\n",
        "        keys = torch.randn(\n",
        "            batch_size, self.num_heads, self.sequence_length, self.head_size\n",
        "        )\n",
        "        attention_weights = self.attention.get_attention_weights(queries, keys)\n",
        "        expected_shape = (\n",
        "            batch_size,\n",
        "            self.num_heads,\n",
        "            self.sequence_length,\n",
        "            self.sequence_length,\n",
        "        )\n",
        "        assert isinstance(attention_weights, torch.Tensor), (\n",
        "            \"The output of \" \"`get_attention_weights` must be a torch.Tensor.\"\n",
        "        )\n",
        "        assert attention_weights.shape == expected_shape, (\n",
        "            \"The shape of your \"\n",
        "            \"attention weights is invalid with `queries` of shape {0}, and \"\n",
        "            \"`keys` of shape {1}.\\n  Got shape: {2}\\n  Expected shape: {3}\\n\"\n",
        "            \"Recall that the attention weights should have shape (batch_size, \"\n",
        "            \"num_heads, sequence_length, sequence_length).\".format(\n",
        "                tuple(queries.shape),\n",
        "                tuple(keys.shape),\n",
        "                tuple(attention_weights.shape),\n",
        "                expected_shape,\n",
        "            )\n",
        "        )\n",
        "    def test_apply_attention_shape(self):\n",
        "        batch_size = 7\n",
        "        queries = torch.randn(\n",
        "            batch_size, self.num_heads, self.sequence_length, self.head_size\n",
        "        )\n",
        "        keys = torch.randn(\n",
        "            batch_size, self.num_heads, self.sequence_length, self.head_size\n",
        "        )\n",
        "        values = torch.randn(\n",
        "            batch_size, self.num_heads, self.sequence_length, self.head_size\n",
        "        )\n",
        "        outputs = self.attention.apply_attention(queries, keys, values)\n",
        "        expected_shape = (\n",
        "            batch_size,\n",
        "            self.sequence_length,\n",
        "            self.num_heads * self.head_size,\n",
        "        )\n",
        "        assert isinstance(outputs, torch.Tensor), (\n",
        "            \"The output of \" \"`apply_attention` must be a torch.Tensor.\"\n",
        "        )\n",
        "        assert outputs.shape == expected_shape, (\n",
        "            \"The shape of the output of \"\n",
        "            \"`apply_attention` is invalid with `queries` of shape {0}, `keys` \"\n",
        "            \"of shape {1}, and `values` of shape {2}.\\n  Got shape: {3}\\n  \"\n",
        "            \"Expected shape: {4}\\nRecall that the attention weights should have \"\n",
        "            \"shape (batch_size, sequence_length, num_heads * head_size).\".format(\n",
        "                tuple(queries.shape),\n",
        "                tuple(keys.shape),\n",
        "                tuple(values.shape),\n",
        "                tuple(outputs.shape),\n",
        "                expected_shape,\n",
        "            )\n",
        "        )\n",
        "    def test_split_heads_shape(self):\n",
        "        batch_size = 7\n",
        "        dim = 23\n",
        "        tensor = torch.randn(batch_size, self.sequence_length, self.num_heads * dim)\n",
        "        output = self.attention.split_heads(tensor)\n",
        "        expected_shape = (batch_size, self.num_heads, self.sequence_length, dim)\n",
        "        assert isinstance(output, torch.Tensor), (\n",
        "            \"The output of `split_heads` \" \"must be a torch.Tensor.\"\n",
        "        )\n",
        "        assert output.shape == expected_shape, (\n",
        "            \"The shape of the output of \"\n",
        "            \"`split_heads` is invalid with `tensor` of shape {0}.\\n  Got shape \"\n",
        "            \"{1}\\n  Expected shape: {2}\\nRecall that the output should have \"\n",
        "            \"shape (batch_size, num_heads, sequence_length, dim)\".format(\n",
        "                tuple(tensor.shape), tuple(output.shape), expected_shape\n",
        "            )\n",
        "        )\n",
        "    def test_merge_heads_shape(self):\n",
        "        batch_size = 7\n",
        "        dim = 23\n",
        "        tensor = torch.randn(batch_size, self.num_heads, self.sequence_length, dim)\n",
        "        output = self.attention.merge_heads(tensor)\n",
        "        expected_shape = (batch_size, self.sequence_length, self.num_heads * dim)\n",
        "        assert isinstance(output, torch.Tensor), (\n",
        "            \"The output of `merge_heads` \" \"must be a torch.Tensor.\"\n",
        "        )\n",
        "        assert output.shape == expected_shape, (\n",
        "            \"The shape of the output of \"\n",
        "            \"`merge_heads` is invalid with `tensor` of shape {0}.\\n  Got shape \"\n",
        "            \"{1}\\n  Expected shape: {2}\\nRecall that the output should have \"\n",
        "            \"shape (batch_size, sequence_length, num_heads * dim)\".format(\n",
        "                tuple(tensor.shape), tuple(output.shape), expected_shape\n",
        "            )\n",
        "        )\n",
        "    def test_forward_shape(self):\n",
        "        batch_size = 7\n",
        "        hidden_states = torch.randn(\n",
        "            batch_size, self.sequence_length, self.num_heads * self.head_size\n",
        "        )\n",
        "        outputs = self.attention(hidden_states, hidden_states, hidden_states)\n",
        "        expected_shape = (\n",
        "            batch_size,\n",
        "            self.sequence_length,\n",
        "            self.num_heads * self.head_size,\n",
        "        )\n",
        "        assert isinstance(outputs, torch.Tensor), (\n",
        "            \"The output of the module \" \"must be a torch.Tensor.\"\n",
        "        )\n",
        "        assert outputs.shape == expected_shape, (\n",
        "            \"The shape of the output of \"\n",
        "            \"the module is invalid with `hidden_sizes` of shape {0}.\\n  Got \"\n",
        "            \"shape {1}\\n  Expected shape: {2}\\nRecall that the output should \"\n",
        "            \"have shape (batch_size, sequence_length, num_heads * head_size)\".format(\n",
        "                tuple(hidden_states.shape), tuple(outputs.shape), expected_shape\n",
        "            )\n",
        "        )"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.8 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "vscode": {
      "interpreter": {
        "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
      }
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Q8PSGSqyr7xX",
        "aG6lzF6Br7xb",
        "ZuJxrseKr7xd",
        "AsdQfb3fr7xe",
        "xrJYvXApr7xf",
        "9qVQPromr7xg",
        "eaktnWgWr7xi",
        "2QV1LRAwr7xj"
      ]
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}